{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\brizio\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(suppress=True)\n",
    "import networkx as nx\n",
    "import random\n",
    "import pandas as pd\n",
    "#import tensorflow as tf\n",
    "#Disable TensorFlow 2 behaviour\n",
    "from sklearn.model_selection import KFold  \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn import preprocessing\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior() \n",
    "import os\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from CASTLE import CASTLE\n",
    "from utils import random_dag, gen_data_nonlinear\n",
    "from signal import signal, SIGINT\n",
    "from sys import exit\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\brizio\\AppData\\Roaming\\Python\\Python37\\site-packages\\networkx\\drawing\\layout.py:950: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  pos = np.row_stack((pos[x] for x in node_list))\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_sz = 5000\n",
    "csv = \"synth_nonlinear.csv\"\n",
    "df = pd.read_csv(csv)\n",
    "df_test = df.iloc[-1000:]\n",
    "df = df.iloc[:dataset_sz]\n",
    "\n",
    "\n",
    "G = nx.DiGraph()\n",
    "for i in range(10):\n",
    "    G.add_node(i)\n",
    "G.add_edge(1,2)\n",
    "G.add_edge(1,3)\n",
    "G.add_edge(1,4)\n",
    "G.add_edge(2,5)\n",
    "G.add_edge(2,0)\n",
    "G.add_edge(3,0)\n",
    "G.add_edge(3,6)\n",
    "G.add_edge(3,7)\n",
    "G.add_edge(6,9)\n",
    "G.add_edge(0,8)\n",
    "G.add_edge(0,9)\n",
    "G\n",
    "\n",
    "pos=nx.planar_layout(G)\n",
    "nx.draw(G,pos)\n",
    "labels={}\n",
    "for i in range(10):\n",
    "    labels[i] = i\n",
    "nx.draw_networkx_labels(G,pos,labels,font_size=16);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['0'] = [int(i > .5) for i in preprocessing.scale(df['0'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    5000.000000\n",
       "mean        2.490209\n",
       "std         1.213778\n",
       "min        -1.437822\n",
       "25%         1.689177\n",
       "50%         2.426209\n",
       "75%         3.205960\n",
       "max        13.763755\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['0'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# G = random_dag(23, 50)\n",
    "# G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_data = False\n",
    "if random_data:\n",
    "    G = random_dag(23, 50)\n",
    "    # G\n",
    "    def handler(signal_received, frame):\n",
    "        # Handle any cleanup here\n",
    "        print('SIGINT or CTRL-C detected. Exiting gracefully')\n",
    "        exit(0)\n",
    "    def str2bool(v):\n",
    "        if isinstance(v, bool):\n",
    "           return v\n",
    "        if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
    "            return True\n",
    "        elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
    "            return False\n",
    "        else:\n",
    "            raise argparse.ArgumentTypeError('Boolean value expected.') \n",
    "\n",
    "    # noise = random.uniform(0.3, 1.0)\n",
    "    # print(\"Setting noise to \", noise)\n",
    "    # df = gen_data_nonlinear(G, SIZE = dataset_sz, var = noise).iloc[:dataset_sz]\n",
    "    # df_test =  gen_data_nonlinear(G, SIZE = int(dataset_sz*0.25), var = noise)\n",
    "    # df\n",
    "\n",
    "    def swap_cols(df, a, b):\n",
    "        df = df.rename(columns = {a : 'temp'})\n",
    "        df = df.rename(columns = {b : a})\n",
    "        return df.rename(columns = {'temp' : b})\n",
    "    def swap_nodes(G, a, b):\n",
    "        newG = nx.relabel_nodes(G, {a : 'temp'})\n",
    "        newG = nx.relabel_nodes(newG, {b : a})\n",
    "        return nx.relabel_nodes(newG, {'temp' : b})\n",
    "\n",
    "    for i in range(len(G.edges())):\n",
    "        if len(list(G.predecessors(i))) > 0:\n",
    "            df = swap_cols(df, str(0), str(i))\n",
    "            df_test = swap_cols(df_test, str(0), str(i))\n",
    "            G = swap_nodes(G, 0, i)\n",
    "            break      \n",
    "\n",
    "    #print(\"Number of parents of G\", len(list(G.predecessors(i))))\n",
    "    print(\"Edges = \", list(G.edges()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset limits are 22.43824804096758 14.264787433454547 12.751701336069463\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "n_folds = 10\n",
    "df = scaler.fit_transform(df)\n",
    "df_test = scaler.transform(df_test)\n",
    "\n",
    "X_test = df_test\n",
    "y_test = df_test[:,0]\n",
    "X_DAG = df\n",
    "\n",
    "kf = KFold(n_splits = n_folds, random_state = 1, shuffle = True)\n",
    "\n",
    "print(\"Dataset limits are\", np.ptp(X_DAG), np.ptp(X_test), np.ptp(y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset limits are 22.43824804096758 14.264787433454547 12.751701336069463\n",
      "fold =  1\n",
      "******* Doing dataset size =  5000 ****************\n",
      "Step 1, Loss= 1.9437  h_value: 0.0005245209\n",
      "Step 2, Loss= 0.7286  h_value: 0.00029850006\n",
      "Step 3, Loss= 0.7263  h_value: 0.00018501282\n",
      "Step 4, Loss= 0.7095  h_value: 0.00013065338\n",
      "Step 5, Loss= 0.7297  h_value: 0.000166893\n",
      "Step 6, Loss= 0.7092  h_value: 0.00015544891\n",
      "Step 7, Loss= 0.7069  h_value: 0.00014686584\n",
      "Step 8, Loss= 0.6903  h_value: 0.00020313263\n",
      "Step 9, Loss= 0.7003  h_value: 0.00016212463\n",
      "Step 10, Loss= 0.6993  h_value: 0.0001411438\n",
      "Step 11, Loss= 0.6923  h_value: 0.0001487732\n",
      "Step 12, Loss= 0.6891  h_value: 0.00018501282\n",
      "Step 13, Loss= 0.6775  h_value: 0.00019741058\n",
      "Step 14, Loss= 0.6952  h_value: 0.0002040863\n",
      "Step 15, Loss= 0.6898  h_value: 0.00020980835\n",
      "Step 16, Loss= 0.6978  h_value: 0.00019550323\n",
      "Step 17, Loss= 0.6906  h_value: 0.00018692017\n",
      "Step 18, Loss= 0.6804  h_value: 0.00022125244\n",
      "Step 19, Loss= 0.6832  h_value: 0.0002708435\n",
      "Step 20, Loss= 0.6829  h_value: 0.0002193451\n",
      "Step 21, Loss= 0.6726  h_value: 0.00018024445\n",
      "Step 22, Loss= 0.6765  h_value: 0.00020217896\n",
      "Step 23, Loss= 0.6832  h_value: 0.00022411346\n",
      "Step 24, Loss= 0.6796  h_value: 0.00025749207\n",
      "Step 25, Loss= 0.6825  h_value: 0.00020694733\n",
      "Step 26, Loss= 0.6840  h_value: 0.00020217896\n",
      "Step 27, Loss= 0.6794  h_value: 0.00021648407\n",
      "Step 28, Loss= 0.6865  h_value: 0.00023555756\n",
      "Step 29, Loss= 0.6790  h_value: 0.00020980835\n",
      "Step 30, Loss= 0.6834  h_value: 0.0001821518\n",
      "Step 31, Loss= 0.6792  h_value: 0.0002040863\n",
      "Step 32, Loss= 0.6718  h_value: 0.00020217896\n",
      "Step 33, Loss= 0.6690  h_value: 0.00023937225\n",
      "Step 34, Loss= 0.6878  h_value: 0.00023174286\n",
      "Step 35, Loss= 0.6857  h_value: 0.00022411346\n",
      "Step 36, Loss= 0.6701  h_value: 0.00021743774\n",
      "Step 37, Loss= 0.6759  h_value: 0.00021648407\n",
      "Step 38, Loss= 0.6730  h_value: 0.00019454956\n",
      "Step 39, Loss= 0.6756  h_value: 0.00022125244\n",
      "Step 40, Loss= 0.6729  h_value: 0.00019645691\n",
      "Step 41, Loss= 0.7033  h_value: 0.00019931793\n",
      "Step 42, Loss= 0.6762  h_value: 0.00017738342\n",
      "Step 43, Loss= 0.6774  h_value: 0.0001821518\n",
      "Step 44, Loss= 0.7053  h_value: 0.000207901\n",
      "Step 45, Loss= 0.6785  h_value: 0.00018596649\n",
      "Step 46, Loss= 0.6765  h_value: 0.0001821518\n",
      "Step 47, Loss= 0.6701  h_value: 0.00016498566\n",
      "Step 48, Loss= 0.6879  h_value: 0.00016403198\n",
      "Step 49, Loss= 0.6773  h_value: 0.00017738342\n",
      "Step 50, Loss= 0.6729  h_value: 0.00015640259\n",
      "Saving model\n",
      "Step 51, Loss= 0.6825  h_value: 0.00020694733\n",
      "Saving model\n",
      "Step 52, Loss= 0.6752  h_value: 0.00019931793\n",
      "Step 53, Loss= 0.6796  h_value: 0.00015735626\n",
      "Step 54, Loss= 0.6707  h_value: 0.00014972687\n",
      "Saving model\n",
      "Step 55, Loss= 0.6747  h_value: 0.00016212463\n",
      "Saving model\n",
      "Step 56, Loss= 0.6738  h_value: 0.00015163422\n",
      "Step 57, Loss= 0.6831  h_value: 0.00015354156\n",
      "Step 58, Loss= 0.6714  h_value: 0.0001745224\n",
      "Step 59, Loss= 0.6809  h_value: 0.00016117096\n",
      "Step 60, Loss= 0.6741  h_value: 0.00015354156\n",
      "Step 61, Loss= 0.6896  h_value: 0.00014686584\n",
      "Step 62, Loss= 0.6758  h_value: 0.0001296997\n",
      "Step 63, Loss= 0.6859  h_value: 0.00015735626\n",
      "Step 64, Loss= 0.6753  h_value: 0.00012588501\n",
      "Step 65, Loss= 0.6733  h_value: 0.00012874603\n",
      "Step 66, Loss= 0.6840  h_value: 0.0001449585\n",
      "Step 67, Loss= 0.6683  h_value: 0.00012207031\n",
      "Step 68, Loss= 0.6717  h_value: 0.000166893\n",
      "Step 69, Loss= 0.6723  h_value: 0.00015830994\n",
      "Step 70, Loss= 0.6683  h_value: 0.00014305115\n",
      "Step 71, Loss= 0.6877  h_value: 0.00013923645\n",
      "Step 72, Loss= 0.6760  h_value: 0.0001335144\n",
      "Step 73, Loss= 0.6849  h_value: 0.0001373291\n",
      "Step 74, Loss= 0.6823  h_value: 0.00017261505\n",
      "Step 75, Loss= 0.6854  h_value: 0.00015068054\n",
      "Step 76, Loss= 0.6748  h_value: 0.00015258789\n",
      "Step 77, Loss= 0.6802  h_value: 0.00012207031\n",
      "Step 78, Loss= 0.6908  h_value: 0.00011253357\n",
      "Step 79, Loss= 0.6717  h_value: 0.00013828278\n",
      "Step 80, Loss= 0.6722  h_value: 0.00016307831\n",
      "Step 81, Loss= 0.6649  h_value: 0.00014591217\n",
      "Step 82, Loss= 0.6888  h_value: 0.00013637543\n",
      "Step 83, Loss= 0.6963  h_value: 0.00013256073\n",
      "Step 84, Loss= 0.6660  h_value: 0.000113487244\n",
      "Step 85, Loss= 0.6794  h_value: 0.00013828278\n",
      "Step 86, Loss= 0.6669  h_value: 0.00013923645\n",
      "Early stopping\n",
      "INFO:tensorflow:Restoring parameters from tmp.ckpt\n",
      "[[0.    0.004 0.021 0.019 0.002 0.043 0.032 0.014 0.048 0.06 ]\n",
      " [0.009 0.    0.062 0.06  0.067 0.008 0.006 0.008 0.005 0.006]\n",
      " [0.098 0.035 0.    0.024 0.019 0.139 0.006 0.005 0.005 0.015]\n",
      " [0.099 0.035 0.038 0.    0.029 0.017 0.093 0.106 0.002 0.013]\n",
      " [0.008 0.036 0.031 0.024 0.    0.005 0.009 0.007 0.    0.01 ]\n",
      " [0.053 0.006 0.02  0.004 0.005 0.    0.008 0.009 0.008 0.013]\n",
      " [0.021 0.006 0.006 0.012 0.005 0.026 0.    0.027 0.004 0.146]\n",
      " [0.024 0.002 0.003 0.012 0.003 0.036 0.029 0.    0.009 0.006]\n",
      " [0.034 0.002 0.004 0.008 0.002 0.011 0.014 0.009 0.    0.017]\n",
      " [0.041 0.003 0.003 0.002 0.004 0.012 0.069 0.002 0.001 0.   ]]\n",
      "MSE =  0.6358877592341934\n",
      "fold =  2\n",
      "******* Doing dataset size =  5000 ****************\n",
      "Destructor Called... Cleaning up\n",
      "Step 1, Loss= 195.0632  h_value: 0.0005245209\n",
      "Step 2, Loss= 5.1630  h_value: 0.17592716\n",
      "Step 3, Loss= 1.4830  h_value: 0.16773033\n",
      "Step 4, Loss= 0.9939  h_value: 0.14396\n",
      "Step 5, Loss= 0.8595  h_value: 0.122675896\n",
      "Step 6, Loss= 0.7925  h_value: 0.10213661\n",
      "Step 7, Loss= 0.7659  h_value: 0.08358765\n",
      "Step 8, Loss= 0.7469  h_value: 0.0680275\n",
      "Step 9, Loss= 0.7489  h_value: 0.054595947\n",
      "Step 10, Loss= 0.7439  h_value: 0.042734146\n",
      "Step 11, Loss= 0.7203  h_value: 0.033686638\n",
      "Step 12, Loss= 0.7129  h_value: 0.025818825\n",
      "Step 13, Loss= 0.7092  h_value: 0.019901276\n",
      "Step 14, Loss= 0.7397  h_value: 0.014936447\n",
      "Step 15, Loss= 0.7254  h_value: 0.011224747\n",
      "Step 16, Loss= 0.7076  h_value: 0.008146286\n",
      "Step 17, Loss= 0.7136  h_value: 0.006070137\n",
      "Step 18, Loss= 0.7373  h_value: 0.004392624\n",
      "Step 19, Loss= 0.6981  h_value: 0.003296852\n",
      "Step 20, Loss= 0.7942  h_value: 0.002565384\n",
      "Step 21, Loss= 0.7171  h_value: 0.0018587112\n",
      "Step 22, Loss= 0.7198  h_value: 0.0015182495\n",
      "Step 23, Loss= 0.7748  h_value: 0.0012054443\n",
      "Step 24, Loss= 0.7428  h_value: 0.0010404587\n",
      "Step 25, Loss= 0.7221  h_value: 0.00096797943\n",
      "Step 26, Loss= 0.7250  h_value: 0.0008535385\n",
      "Step 27, Loss= 0.7144  h_value: 0.0006313324\n",
      "Step 28, Loss= 0.7327  h_value: 0.00056648254\n",
      "Step 29, Loss= 0.7468  h_value: 0.00057697296\n",
      "Step 30, Loss= 0.7700  h_value: 0.0004720688\n",
      "Step 31, Loss= 0.7051  h_value: 0.00043964386\n",
      "Step 32, Loss= 0.7429  h_value: 0.00048065186\n",
      "Step 33, Loss= 0.7200  h_value: 0.00037574768\n",
      "Step 34, Loss= 0.7392  h_value: 0.00043582916\n",
      "Step 35, Loss= 0.7131  h_value: 0.0003604889\n",
      "Step 36, Loss= 0.7307  h_value: 0.00039196014\n",
      "Step 37, Loss= 0.7110  h_value: 0.00036525726\n",
      "Step 38, Loss= 0.7417  h_value: 0.00032424927\n",
      "Step 39, Loss= 0.7481  h_value: 0.00034427643\n",
      "Step 40, Loss= 0.7166  h_value: 0.0003080368\n",
      "Step 41, Loss= 0.7134  h_value: 0.0003042221\n",
      "Step 42, Loss= 0.7165  h_value: 0.00025177002\n",
      "Step 43, Loss= 0.7232  h_value: 0.00030899048\n",
      "Step 44, Loss= 0.7303  h_value: 0.00026226044\n",
      "Step 45, Loss= 0.7411  h_value: 0.00024700165\n",
      "Step 46, Loss= 0.7300  h_value: 0.0002412796\n",
      "Step 47, Loss= 0.7004  h_value: 0.00027275085\n",
      "Step 48, Loss= 0.7076  h_value: 0.0002412796\n",
      "Step 49, Loss= 0.7373  h_value: 0.00032234192\n",
      "Step 50, Loss= 0.7078  h_value: 0.00025081635\n",
      "Saving model\n",
      "Step 51, Loss= 0.7048  h_value: 0.00029087067\n",
      "Step 52, Loss= 0.7355  h_value: 0.00022697449\n",
      "Step 53, Loss= 0.7534  h_value: 0.00022125244\n",
      "Step 54, Loss= 0.7414  h_value: 0.00019073486\n",
      "Step 55, Loss= 0.7120  h_value: 0.00016880035\n",
      "Step 56, Loss= 0.8061  h_value: 0.00018310547\n",
      "Step 57, Loss= 0.7032  h_value: 0.00023651123\n",
      "Step 58, Loss= 0.7083  h_value: 0.00022220612\n",
      "Step 59, Loss= 0.7316  h_value: 0.00019931793\n",
      "Step 60, Loss= 0.7238  h_value: 0.00020694733\n",
      "Step 61, Loss= 0.7626  h_value: 0.00022125244\n",
      "Step 62, Loss= 0.7115  h_value: 0.00018787384\n",
      "Saving model\n",
      "Step 63, Loss= 0.7074  h_value: 0.00018787384\n",
      "Step 64, Loss= 0.7200  h_value: 0.0002784729\n",
      "Step 65, Loss= 0.7075  h_value: 0.00019264221\n",
      "Step 66, Loss= 0.7115  h_value: 0.0002040863\n",
      "Step 67, Loss= 0.7196  h_value: 0.00020599365\n",
      "Step 68, Loss= 0.7095  h_value: 0.00017642975\n",
      "Step 69, Loss= 0.7245  h_value: 0.00016784668\n",
      "Step 70, Loss= 0.7077  h_value: 0.00019454956\n",
      "Step 71, Loss= 0.6981  h_value: 0.00016021729\n",
      "Step 72, Loss= 0.7374  h_value: 0.00016212463\n",
      "Step 73, Loss= 0.7180  h_value: 0.0001335144\n",
      "Step 74, Loss= 0.7212  h_value: 0.00015449524\n",
      "Step 75, Loss= 0.7069  h_value: 0.00016880035\n",
      "Step 76, Loss= 0.7260  h_value: 0.0001783371\n",
      "Step 77, Loss= 0.7203  h_value: 0.00015830994\n",
      "Step 78, Loss= 0.7424  h_value: 0.00015163422\n",
      "Step 79, Loss= 0.7128  h_value: 0.00014209747\n",
      "Step 80, Loss= 0.7045  h_value: 0.00014972687\n",
      "Step 81, Loss= 0.7746  h_value: 0.00016498566\n",
      "Step 82, Loss= 0.7184  h_value: 0.0001707077\n",
      "Step 83, Loss= 0.7054  h_value: 0.00019550323\n",
      "Step 84, Loss= 0.7043  h_value: 0.00018310547\n",
      "Step 85, Loss= 0.7113  h_value: 0.00021362305\n",
      "Step 86, Loss= 0.7067  h_value: 0.00016403198\n",
      "Step 87, Loss= 0.7198  h_value: 0.00022602081\n",
      "Step 88, Loss= 0.7027  h_value: 0.00018024445\n",
      "Saving model\n",
      "Step 89, Loss= 0.7044  h_value: 0.00015449524\n",
      "Step 90, Loss= 0.7084  h_value: 0.00016212463\n",
      "Step 91, Loss= 0.7487  h_value: 0.00011920929\n",
      "Step 92, Loss= 0.7293  h_value: 0.00016593933\n",
      "Saving model\n",
      "Step 93, Loss= 0.7132  h_value: 0.0001449585\n",
      "Step 94, Loss= 0.7258  h_value: 0.00016117096\n",
      "Step 95, Loss= 0.6981  h_value: 0.0001487732\n",
      "Step 96, Loss= 0.7262  h_value: 0.00013446808\n",
      "Step 97, Loss= 0.7351  h_value: 0.00016403198\n",
      "Step 98, Loss= 0.7053  h_value: 0.00013542175\n",
      "Step 99, Loss= 0.7099  h_value: 0.00017356873\n",
      "Step 100, Loss= 0.7190  h_value: 0.00015544891\n",
      "Step 101, Loss= 0.7201  h_value: 0.00014400482\n",
      "Step 102, Loss= 0.7188  h_value: 0.00015640259\n",
      "Step 103, Loss= 0.7109  h_value: 0.0001487732\n",
      "Step 104, Loss= 0.7070  h_value: 0.00017261505\n",
      "Step 105, Loss= 0.7030  h_value: 0.00019073486\n",
      "Step 106, Loss= 0.7033  h_value: 0.00019264221\n",
      "Step 107, Loss= 0.7128  h_value: 0.00015830994\n",
      "Step 108, Loss= 0.7133  h_value: 0.00014305115\n",
      "Step 109, Loss= 0.7635  h_value: 0.00012779236\n",
      "Step 110, Loss= 0.7095  h_value: 0.00016498566\n",
      "Step 111, Loss= 0.7096  h_value: 0.00015354156\n",
      "Step 112, Loss= 0.7295  h_value: 0.0001296997\n",
      "Step 113, Loss= 0.7047  h_value: 0.00018024445\n",
      "Step 114, Loss= 0.7214  h_value: 0.00018787384\n",
      "Step 115, Loss= 0.7283  h_value: 0.000166893\n",
      "Step 116, Loss= 0.7071  h_value: 0.00019836426\n",
      "Step 117, Loss= 0.7194  h_value: 0.0001373291\n",
      "Step 118, Loss= 0.7120  h_value: 0.00016307831\n",
      "Step 119, Loss= 0.7272  h_value: 0.00014781952\n",
      "Step 120, Loss= 0.7163  h_value: 0.00019168854\n",
      "Step 121, Loss= 0.7298  h_value: 0.00018596649\n",
      "Step 122, Loss= 0.7116  h_value: 0.00018119812\n",
      "Step 123, Loss= 0.7275  h_value: 0.00016117096\n",
      "Early stopping\n",
      "INFO:tensorflow:Restoring parameters from tmp.ckpt\n",
      "[[0.    0.007 0.01  0.009 0.002 0.045 0.02  0.012 0.049 0.013]\n",
      " [0.013 0.    0.065 0.058 0.057 0.007 0.004 0.005 0.002 0.008]\n",
      " [0.088 0.048 0.    0.036 0.034 0.157 0.014 0.003 0.009 0.005]\n",
      " [0.084 0.059 0.043 0.    0.041 0.006 0.127 0.146 0.01  0.006]\n",
      " [0.005 0.054 0.033 0.033 0.    0.003 0.006 0.004 0.004 0.007]\n",
      " [0.069 0.004 0.007 0.006 0.004 0.    0.01  0.01  0.006 0.008]\n",
      " [0.024 0.003 0.002 0.011 0.004 0.009 0.    0.012 0.008 0.168]\n",
      " [0.022 0.01  0.003 0.003 0.005 0.011 0.016 0.    0.003 0.01 ]\n",
      " [0.018 0.005 0.003 0.003 0.004 0.009 0.011 0.004 0.    0.004]\n",
      " [0.013 0.003 0.002 0.003 0.003 0.005 0.057 0.009 0.009 0.   ]]\n",
      "MSE =  0.6828552418099471\n",
      "0.6593715005220702 0.023483741287876836\n",
      "fold =  3\n",
      "******* Doing dataset size =  5000 ****************\n",
      "Destructor Called... Cleaning up\n",
      "Step 1, Loss= 1.1807  h_value: 0.0005245209\n",
      "Step 2, Loss= 0.7303  h_value: 0.00012874603\n",
      "Step 3, Loss= 0.7176  h_value: 9.6321106e-05\n",
      "Step 4, Loss= 0.7751  h_value: 0.0001335144\n",
      "Step 5, Loss= 0.7191  h_value: 0.000113487244\n",
      "Step 6, Loss= 0.7059  h_value: 0.00012683868\n",
      "Step 7, Loss= 0.7010  h_value: 0.00013256073\n",
      "Step 8, Loss= 0.6952  h_value: 0.00013637543\n",
      "Step 9, Loss= 0.7388  h_value: 0.00011444092\n",
      "Step 10, Loss= 0.6993  h_value: 0.00011444092\n",
      "Step 11, Loss= 0.6863  h_value: 0.00012683868\n",
      "Step 12, Loss= 0.6872  h_value: 0.000120162964\n",
      "Step 13, Loss= 0.7044  h_value: 0.00012207031\n",
      "Step 14, Loss= 0.6841  h_value: 0.00015163422\n",
      "Step 15, Loss= 0.6958  h_value: 0.00013637543\n",
      "Step 16, Loss= 0.6926  h_value: 0.00011062622\n",
      "Step 17, Loss= 0.7085  h_value: 0.00014972687\n",
      "Step 18, Loss= 0.6811  h_value: 0.0001335144\n",
      "Step 19, Loss= 0.6816  h_value: 0.00010681152\n",
      "Step 20, Loss= 0.6885  h_value: 0.0001296997\n",
      "Step 21, Loss= 0.6850  h_value: 0.00013542175\n",
      "Step 22, Loss= 0.7255  h_value: 0.00017738342\n",
      "Step 23, Loss= 0.6823  h_value: 0.0001707077\n",
      "Step 24, Loss= 0.7157  h_value: 0.00016117096\n",
      "Step 25, Loss= 0.6898  h_value: 0.00016498566\n",
      "Step 26, Loss= 0.6825  h_value: 0.0001411438\n",
      "Step 27, Loss= 0.6811  h_value: 0.00011062622\n",
      "Step 28, Loss= 0.7035  h_value: 0.00018596649\n",
      "Step 29, Loss= 0.6835  h_value: 0.00015830994\n",
      "Step 30, Loss= 0.6876  h_value: 0.00014591217\n",
      "Step 31, Loss= 0.7048  h_value: 0.0001373291\n",
      "Step 32, Loss= 0.6825  h_value: 0.00016593933\n",
      "Step 33, Loss= 0.6878  h_value: 0.00014686584\n",
      "Step 34, Loss= 0.6858  h_value: 0.0001411438\n",
      "Step 35, Loss= 0.6797  h_value: 0.0001373291\n",
      "Step 36, Loss= 0.6906  h_value: 0.0001335144\n",
      "Step 37, Loss= 0.6792  h_value: 0.00015640259\n",
      "Step 38, Loss= 0.6891  h_value: 0.00012874603\n",
      "Step 39, Loss= 0.7042  h_value: 0.00013446808\n",
      "Step 40, Loss= 0.6780  h_value: 0.00014591217\n",
      "Step 41, Loss= 0.6879  h_value: 0.00012874603\n",
      "Step 42, Loss= 0.6878  h_value: 0.00010681152\n",
      "Step 43, Loss= 0.6961  h_value: 0.000118255615\n",
      "Step 44, Loss= 0.6879  h_value: 0.000120162964\n",
      "Step 45, Loss= 0.7056  h_value: 0.00011253357\n",
      "Step 46, Loss= 0.6892  h_value: 9.918213e-05\n",
      "Step 47, Loss= 0.6797  h_value: 0.00011253357\n",
      "Step 48, Loss= 0.7259  h_value: 0.00012207031\n",
      "Step 49, Loss= 0.6839  h_value: 0.00012302399\n",
      "Step 50, Loss= 0.6869  h_value: 0.00012111664\n",
      "Saving model\n",
      "Step 51, Loss= 0.7038  h_value: 0.0001335144\n",
      "Step 52, Loss= 0.6851  h_value: 0.00012207031\n",
      "Saving model\n",
      "Step 53, Loss= 0.6909  h_value: 0.00012397766\n",
      "Saving model\n",
      "Step 54, Loss= 0.6793  h_value: 0.00010585785\n",
      "Step 55, Loss= 0.6864  h_value: 0.0001039505\n",
      "Step 56, Loss= 0.6847  h_value: 0.0001039505\n",
      "Step 57, Loss= 0.6792  h_value: 0.00011062622\n",
      "Step 58, Loss= 0.6796  h_value: 0.00011920929\n",
      "Step 59, Loss= 0.6793  h_value: 0.000111579895\n",
      "Step 60, Loss= 0.6780  h_value: 0.000104904175\n",
      "Step 61, Loss= 0.6916  h_value: 0.00010871887\n",
      "Step 62, Loss= 0.6785  h_value: 0.00010204315\n",
      "Step 63, Loss= 0.6740  h_value: 0.00010108948\n",
      "Step 64, Loss= 0.6841  h_value: 9.250641e-05\n",
      "Step 65, Loss= 0.6871  h_value: 9.346008e-05\n",
      "Step 66, Loss= 0.6842  h_value: 8.9645386e-05\n",
      "Step 67, Loss= 0.6788  h_value: 8.583069e-05\n",
      "Step 68, Loss= 0.6804  h_value: 0.00010204315\n",
      "Step 69, Loss= 0.6854  h_value: 0.000102996826\n",
      "Step 70, Loss= 0.6907  h_value: 0.000102996826\n",
      "Step 71, Loss= 0.6922  h_value: 9.1552734e-05\n",
      "Step 72, Loss= 0.6954  h_value: 9.536743e-05\n",
      "Step 73, Loss= 0.6758  h_value: 0.0001001358\n",
      "Step 74, Loss= 0.6849  h_value: 9.8228455e-05\n",
      "Step 75, Loss= 0.6850  h_value: 0.000104904175\n",
      "Step 76, Loss= 0.6855  h_value: 8.583069e-05\n",
      "Step 77, Loss= 0.6921  h_value: 8.2969666e-05\n",
      "Step 78, Loss= 0.7037  h_value: 0.000111579895\n",
      "Step 79, Loss= 0.6818  h_value: 0.00011634827\n",
      "Step 80, Loss= 0.6859  h_value: 8.2969666e-05\n",
      "Step 81, Loss= 0.6944  h_value: 9.918213e-05\n",
      "Step 82, Loss= 0.6769  h_value: 8.9645386e-05\n",
      "Step 83, Loss= 0.6825  h_value: 0.00010108948\n",
      "Step 84, Loss= 0.7013  h_value: 8.773804e-05\n",
      "Early stopping\n",
      "INFO:tensorflow:Restoring parameters from tmp.ckpt\n",
      "[[0.    0.004 0.017 0.013 0.002 0.027 0.045 0.01  0.039 0.052]\n",
      " [0.006 0.    0.067 0.054 0.054 0.006 0.008 0.009 0.003 0.008]\n",
      " [0.092 0.032 0.    0.027 0.017 0.122 0.002 0.006 0.005 0.008]\n",
      " [0.083 0.034 0.025 0.    0.028 0.012 0.098 0.119 0.002 0.003]\n",
      " [0.008 0.036 0.021 0.029 0.    0.006 0.015 0.004 0.003 0.007]\n",
      " [0.04  0.003 0.012 0.002 0.004 0.    0.011 0.005 0.006 0.007]\n",
      " [0.019 0.006 0.002 0.015 0.004 0.009 0.    0.012 0.007 0.114]\n",
      " [0.023 0.01  0.007 0.017 0.005 0.01  0.025 0.    0.003 0.006]\n",
      " [0.033 0.002 0.005 0.004 0.003 0.006 0.019 0.008 0.    0.014]\n",
      " [0.023 0.002 0.004 0.003 0.007 0.004 0.077 0.003 0.008 0.   ]]\n",
      "MSE =  0.6495416770855457\n",
      "0.6560948927098953 0.019726371201014933\n",
      "fold =  4\n",
      "******* Doing dataset size =  5000 ****************\n",
      "Destructor Called... Cleaning up\n",
      "Step 1, Loss= 13.8459  h_value: 0.0005245209\n",
      "Step 2, Loss= 0.7841  h_value: 0.004149437\n",
      "Step 3, Loss= 0.7390  h_value: 0.0017156601\n",
      "Step 4, Loss= 0.7365  h_value: 0.0008354187\n",
      "Step 5, Loss= 0.7385  h_value: 0.00059223175\n",
      "Step 6, Loss= 0.7875  h_value: 0.00044727325\n",
      "Step 7, Loss= 0.7227  h_value: 0.00046920776\n",
      "Step 8, Loss= 0.7277  h_value: 0.0003681183\n",
      "Step 9, Loss= 0.7291  h_value: 0.00032234192\n",
      "Step 10, Loss= 0.7175  h_value: 0.00032424927\n",
      "Step 11, Loss= 0.7009  h_value: 0.00025844574\n",
      "Step 12, Loss= 0.7258  h_value: 0.00024414062\n",
      "Step 13, Loss= 0.7294  h_value: 0.00025844574\n",
      "Step 14, Loss= 0.7039  h_value: 0.00021839142\n",
      "Step 15, Loss= 0.7101  h_value: 0.00023841858\n",
      "Step 16, Loss= 0.7158  h_value: 0.0002374649\n",
      "Step 17, Loss= 0.7123  h_value: 0.00021076202\n",
      "Step 18, Loss= 0.7157  h_value: 0.0002374649\n",
      "Step 19, Loss= 0.7040  h_value: 0.0002374649\n",
      "Step 20, Loss= 0.7499  h_value: 0.00022697449\n",
      "Step 21, Loss= 0.7415  h_value: 0.00026798248\n",
      "Step 22, Loss= 0.7398  h_value: 0.00024223328\n",
      "Step 23, Loss= 0.6970  h_value: 0.00024414062\n",
      "Step 24, Loss= 0.6969  h_value: 0.00023269653\n",
      "Step 25, Loss= 0.7076  h_value: 0.0002155304\n",
      "Step 26, Loss= 0.7227  h_value: 0.00022506714\n",
      "Step 27, Loss= 0.6962  h_value: 0.00020122528\n",
      "Step 28, Loss= 0.6959  h_value: 0.00023460388\n",
      "Step 29, Loss= 0.6966  h_value: 0.00022983551\n",
      "Step 30, Loss= 0.7135  h_value: 0.00022888184\n",
      "Step 31, Loss= 0.6981  h_value: 0.00023651123\n",
      "Step 32, Loss= 0.7433  h_value: 0.00017547607\n",
      "Step 33, Loss= 0.6941  h_value: 0.000207901\n",
      "Step 34, Loss= 0.6963  h_value: 0.00018119812\n",
      "Step 35, Loss= 0.7034  h_value: 0.00019645691\n",
      "Step 36, Loss= 0.6949  h_value: 0.00019836426\n",
      "Step 37, Loss= 0.6991  h_value: 0.00018787384\n",
      "Step 38, Loss= 0.7083  h_value: 0.00019741058\n",
      "Step 39, Loss= 0.6998  h_value: 0.0001821518\n",
      "Step 40, Loss= 0.7145  h_value: 0.00018119812\n",
      "Step 41, Loss= 0.6995  h_value: 0.00017929077\n",
      "Step 42, Loss= 0.6955  h_value: 0.00018501282\n",
      "Step 43, Loss= 0.6970  h_value: 0.00019264221\n",
      "Step 44, Loss= 0.7074  h_value: 0.00019931793\n",
      "Step 45, Loss= 0.6894  h_value: 0.0001707077\n",
      "Step 46, Loss= 0.7204  h_value: 0.00018310547\n",
      "Step 47, Loss= 0.6926  h_value: 0.0001707077\n",
      "Step 48, Loss= 0.6930  h_value: 0.00019741058\n",
      "Step 49, Loss= 0.6915  h_value: 0.00020122528\n",
      "Step 50, Loss= 0.6941  h_value: 0.00018310547\n",
      "Saving model\n",
      "Step 51, Loss= 0.6973  h_value: 0.00019931793\n",
      "Saving model\n",
      "Step 52, Loss= 0.6875  h_value: 0.00019550323\n",
      "Step 53, Loss= 0.6857  h_value: 0.00018119812\n",
      "Saving model\n",
      "Step 54, Loss= 0.6944  h_value: 0.00018787384\n",
      "Step 55, Loss= 0.7071  h_value: 0.0001821518\n",
      "Step 56, Loss= 0.6951  h_value: 0.00018596649\n",
      "Step 57, Loss= 0.7016  h_value: 0.00021076202\n",
      "Step 58, Loss= 0.6897  h_value: 0.00017642975\n",
      "Step 59, Loss= 0.6974  h_value: 0.00019359589\n",
      "Step 60, Loss= 0.6969  h_value: 0.00019836426\n",
      "Step 61, Loss= 0.7010  h_value: 0.00022125244\n",
      "Step 62, Loss= 0.6929  h_value: 0.000207901\n",
      "Step 63, Loss= 0.6931  h_value: 0.00017261505\n",
      "Step 64, Loss= 0.6946  h_value: 0.00017929077\n",
      "Step 65, Loss= 0.6868  h_value: 0.00019550323\n",
      "Step 66, Loss= 0.7021  h_value: 0.00018882751\n",
      "Step 67, Loss= 0.6930  h_value: 0.00014686584\n",
      "Step 68, Loss= 0.6917  h_value: 0.00017356873\n",
      "Step 69, Loss= 0.6889  h_value: 0.00014305115\n",
      "Step 70, Loss= 0.6949  h_value: 0.00017547607\n",
      "Step 71, Loss= 0.6931  h_value: 0.00016784668\n",
      "Step 72, Loss= 0.6981  h_value: 0.00016880035\n",
      "Step 73, Loss= 0.6905  h_value: 0.00016880035\n",
      "Step 74, Loss= 0.6938  h_value: 0.00017929077\n",
      "Step 75, Loss= 0.6993  h_value: 0.00019168854\n",
      "Step 76, Loss= 0.7024  h_value: 0.00018405914\n",
      "Step 77, Loss= 0.6933  h_value: 0.00015354156\n",
      "Step 78, Loss= 0.7092  h_value: 0.00016784668\n",
      "Step 79, Loss= 0.7030  h_value: 0.00016307831\n",
      "Step 80, Loss= 0.7019  h_value: 0.00014972687\n",
      "Step 81, Loss= 0.6881  h_value: 0.00015735626\n",
      "Step 82, Loss= 0.6898  h_value: 0.00016880035\n",
      "Step 83, Loss= 0.6925  h_value: 0.00016498566\n",
      "Step 84, Loss= 0.6901  h_value: 0.00016307831\n",
      "Early stopping\n",
      "INFO:tensorflow:Restoring parameters from tmp.ckpt\n",
      "[[0.    0.002 0.021 0.011 0.003 0.037 0.047 0.015 0.046 0.062]\n",
      " [0.006 0.    0.073 0.076 0.081 0.005 0.008 0.01  0.011 0.006]\n",
      " [0.11  0.042 0.    0.029 0.024 0.149 0.012 0.017 0.013 0.006]\n",
      " [0.08  0.049 0.034 0.    0.033 0.016 0.106 0.129 0.009 0.006]\n",
      " [0.004 0.051 0.023 0.031 0.    0.006 0.017 0.015 0.012 0.006]\n",
      " [0.042 0.004 0.014 0.007 0.003 0.    0.009 0.008 0.007 0.008]\n",
      " [0.016 0.003 0.003 0.008 0.004 0.013 0.    0.016 0.008 0.145]\n",
      " [0.018 0.003 0.005 0.009 0.005 0.013 0.025 0.    0.009 0.006]\n",
      " [0.014 0.002 0.003 0.006 0.007 0.009 0.014 0.011 0.    0.01 ]\n",
      " [0.027 0.003 0.002 0.003 0.002 0.007 0.077 0.006 0.008 0.   ]]\n",
      "MSE =  0.6817738418771034\n",
      "0.6625146300016973 0.020383482804894604\n",
      "fold =  5\n",
      "******* Doing dataset size =  5000 ****************\n",
      "Destructor Called... Cleaning up\n",
      "Step 1, Loss= 5.5341  h_value: 0.0005245209\n",
      "Step 2, Loss= 0.7219  h_value: 0.0008687973\n",
      "Step 3, Loss= 0.7056  h_value: 0.0003862381\n",
      "Step 4, Loss= 0.7340  h_value: 0.00026893616\n",
      "Step 5, Loss= 0.7100  h_value: 0.00020694733\n",
      "Step 6, Loss= 0.6994  h_value: 0.00018119812\n",
      "Step 7, Loss= 0.6946  h_value: 0.00015735626\n",
      "Step 8, Loss= 0.7373  h_value: 0.00017261505\n",
      "Step 9, Loss= 0.7027  h_value: 0.00015640259\n",
      "Step 10, Loss= 0.7067  h_value: 0.00015926361\n",
      "Step 11, Loss= 0.6908  h_value: 0.00016117096\n",
      "Step 12, Loss= 0.7127  h_value: 0.00016784668\n",
      "Step 13, Loss= 0.6913  h_value: 0.00016498566\n",
      "Step 14, Loss= 0.6887  h_value: 0.00017356873\n",
      "Step 15, Loss= 0.7115  h_value: 0.00015544891\n",
      "Step 16, Loss= 0.6888  h_value: 0.00018787384\n",
      "Step 17, Loss= 0.6857  h_value: 0.00020694733\n",
      "Step 18, Loss= 0.7026  h_value: 0.0001707077\n",
      "Step 19, Loss= 0.7047  h_value: 0.00019168854\n",
      "Step 20, Loss= 0.6968  h_value: 0.0002040863\n",
      "Step 21, Loss= 0.7000  h_value: 0.00017642975\n",
      "Step 22, Loss= 0.7085  h_value: 0.00021839142\n",
      "Step 23, Loss= 0.6974  h_value: 0.0002155304\n",
      "Step 24, Loss= 0.6925  h_value: 0.00017929077\n",
      "Step 25, Loss= 0.6897  h_value: 0.00016975403\n",
      "Step 26, Loss= 0.6918  h_value: 0.00018692017\n",
      "Step 27, Loss= 0.6871  h_value: 0.00018405914\n",
      "Step 28, Loss= 0.6924  h_value: 0.00021266937\n",
      "Step 29, Loss= 0.6870  h_value: 0.00024604797\n",
      "Step 30, Loss= 0.6885  h_value: 0.00018692017\n",
      "Step 31, Loss= 0.6902  h_value: 0.00018882751\n",
      "Step 32, Loss= 0.6929  h_value: 0.0001707077\n",
      "Step 33, Loss= 0.6813  h_value: 0.000207901\n",
      "Step 34, Loss= 0.6861  h_value: 0.00020599365\n",
      "Step 35, Loss= 0.6821  h_value: 0.00019168854\n",
      "Step 36, Loss= 0.6869  h_value: 0.00018978119\n",
      "Step 37, Loss= 0.6838  h_value: 0.0002193451\n",
      "Step 38, Loss= 0.6950  h_value: 0.00018882751\n",
      "Step 39, Loss= 0.6927  h_value: 0.00021266937\n",
      "Step 40, Loss= 0.6774  h_value: 0.00015544891\n",
      "Step 41, Loss= 0.6766  h_value: 0.0002193451\n",
      "Step 42, Loss= 0.6865  h_value: 0.00022411346\n",
      "Step 43, Loss= 0.6853  h_value: 0.00019073486\n",
      "Step 44, Loss= 0.6879  h_value: 0.0002002716\n",
      "Step 45, Loss= 0.6893  h_value: 0.00019931793\n",
      "Step 46, Loss= 0.6823  h_value: 0.00018310547\n",
      "Step 47, Loss= 0.6812  h_value: 0.00015830994\n",
      "Step 48, Loss= 0.6928  h_value: 0.00012493134\n",
      "Step 49, Loss= 0.6902  h_value: 0.00018596649\n",
      "Step 50, Loss= 0.6910  h_value: 0.00016975403\n",
      "Saving model\n",
      "Step 51, Loss= 0.6786  h_value: 0.00016307831\n",
      "Step 52, Loss= 0.6845  h_value: 0.0001707077\n",
      "Step 53, Loss= 0.6969  h_value: 0.00016880035\n",
      "Step 54, Loss= 0.6792  h_value: 0.00020980835\n",
      "Step 55, Loss= 0.6934  h_value: 0.0001745224\n",
      "Step 56, Loss= 0.6946  h_value: 0.00019931793\n",
      "Step 57, Loss= 0.6989  h_value: 0.00018405914\n",
      "Step 58, Loss= 0.6751  h_value: 0.00019550323\n",
      "Step 59, Loss= 0.6707  h_value: 0.0002193451\n",
      "Step 60, Loss= 0.6872  h_value: 0.00020503998\n",
      "Step 61, Loss= 0.6817  h_value: 0.00021839142\n",
      "Step 62, Loss= 0.6771  h_value: 0.00018405914\n",
      "Step 63, Loss= 0.6783  h_value: 0.00019359589\n",
      "Step 64, Loss= 0.6797  h_value: 0.00017929077\n",
      "Step 65, Loss= 0.6880  h_value: 0.00023555756\n",
      "Step 66, Loss= 0.6840  h_value: 0.00019073486\n",
      "Step 67, Loss= 0.6745  h_value: 0.00019550323\n",
      "Step 68, Loss= 0.6752  h_value: 0.00016117096\n",
      "Saving model\n",
      "Step 69, Loss= 0.6748  h_value: 0.00016021729\n",
      "Step 70, Loss= 0.6925  h_value: 0.00016975403\n",
      "Step 71, Loss= 0.6774  h_value: 0.00015544891\n",
      "Step 72, Loss= 0.6835  h_value: 0.00015926361\n",
      "Step 73, Loss= 0.6706  h_value: 0.00018787384\n",
      "Step 74, Loss= 0.6931  h_value: 0.0001487732\n",
      "Step 75, Loss= 0.6830  h_value: 0.00017738342\n",
      "Step 76, Loss= 0.6774  h_value: 0.00011920929\n",
      "Step 77, Loss= 0.6902  h_value: 0.00016307831\n",
      "Step 78, Loss= 0.6777  h_value: 0.00016593933\n",
      "Step 79, Loss= 0.6753  h_value: 0.00014781952\n",
      "Saving model\n",
      "Step 80, Loss= 0.6733  h_value: 0.00016403198\n",
      "Step 81, Loss= 0.6821  h_value: 0.0001707077\n",
      "Step 82, Loss= 0.6841  h_value: 0.00019741058\n",
      "Step 83, Loss= 0.6916  h_value: 0.00019836426\n",
      "Step 84, Loss= 0.6878  h_value: 0.00017642975\n",
      "Step 85, Loss= 0.6795  h_value: 0.00015068054\n",
      "Step 86, Loss= 0.6837  h_value: 0.00015926361\n",
      "Step 87, Loss= 0.6682  h_value: 0.00016307831\n",
      "Step 88, Loss= 0.6708  h_value: 0.00014781952\n",
      "Step 89, Loss= 0.6837  h_value: 0.00015735626\n",
      "Step 90, Loss= 0.6977  h_value: 0.00015258789\n",
      "Saving model\n",
      "Step 91, Loss= 0.6700  h_value: 0.00014209747\n",
      "Step 92, Loss= 0.6732  h_value: 0.00015830994\n",
      "Step 93, Loss= 0.6801  h_value: 0.00014019012\n",
      "Step 94, Loss= 0.7112  h_value: 0.00013637543\n",
      "Step 95, Loss= 0.6735  h_value: 0.00012111664\n",
      "Step 96, Loss= 0.6840  h_value: 0.00014209747\n",
      "Step 97, Loss= 0.6687  h_value: 0.00014209747\n",
      "Step 98, Loss= 0.6763  h_value: 0.00012779236\n",
      "Step 99, Loss= 0.7137  h_value: 0.00013637543\n",
      "Step 100, Loss= 0.6723  h_value: 0.00014400482\n",
      "Step 101, Loss= 0.6990  h_value: 0.00015830994\n",
      "Step 102, Loss= 0.6728  h_value: 0.00015258789\n",
      "Step 103, Loss= 0.7056  h_value: 0.00012779236\n",
      "Step 104, Loss= 0.6769  h_value: 0.00015544891\n",
      "Step 105, Loss= 0.6751  h_value: 0.00012397766\n",
      "Step 106, Loss= 0.6953  h_value: 0.00014400482\n",
      "Step 107, Loss= 0.6780  h_value: 0.0001296997\n",
      "Step 108, Loss= 0.6832  h_value: 0.0001411438\n",
      "Step 109, Loss= 0.6976  h_value: 0.0001296997\n",
      "Step 110, Loss= 0.6902  h_value: 0.00011539459\n",
      "Step 111, Loss= 0.6807  h_value: 0.00015926361\n",
      "Step 112, Loss= 0.6828  h_value: 0.00014972687\n",
      "Step 113, Loss= 0.6914  h_value: 0.00013446808\n",
      "Step 114, Loss= 0.6998  h_value: 0.0001411438\n",
      "Step 115, Loss= 0.6972  h_value: 0.00011730194\n",
      "Step 116, Loss= 0.6714  h_value: 0.0001335144\n",
      "Step 117, Loss= 0.6706  h_value: 0.00015258789\n",
      "Step 118, Loss= 0.6826  h_value: 0.00014591217\n",
      "Step 119, Loss= 0.6856  h_value: 0.00011634827\n",
      "Step 120, Loss= 0.6809  h_value: 0.000166893\n",
      "Step 121, Loss= 0.7032  h_value: 0.00014591217\n",
      "Early stopping\n",
      "INFO:tensorflow:Restoring parameters from tmp.ckpt\n",
      "[[0.    0.004 0.017 0.01  0.002 0.025 0.034 0.01  0.059 0.054]\n",
      " [0.007 0.    0.076 0.058 0.068 0.011 0.011 0.007 0.004 0.01 ]\n",
      " [0.11  0.04  0.    0.019 0.015 0.132 0.014 0.003 0.004 0.005]\n",
      " [0.091 0.036 0.023 0.    0.024 0.009 0.109 0.114 0.006 0.007]\n",
      " [0.009 0.038 0.013 0.019 0.    0.01  0.016 0.005 0.002 0.006]\n",
      " [0.024 0.004 0.018 0.004 0.004 0.    0.014 0.011 0.014 0.006]\n",
      " [0.025 0.004 0.006 0.009 0.003 0.006 0.    0.01  0.007 0.135]\n",
      " [0.016 0.003 0.009 0.013 0.003 0.012 0.015 0.    0.003 0.01 ]\n",
      " [0.027 0.005 0.009 0.006 0.004 0.012 0.016 0.005 0.    0.002]\n",
      " [0.029 0.006 0.011 0.003 0.003 0.003 0.075 0.005 0.006 0.   ]]\n",
      "MSE =  0.6255133190422947\n",
      "0.6551143678098168 0.023482857982797867\n",
      "fold =  6\n",
      "******* Doing dataset size =  5000 ****************\n",
      "Destructor Called... Cleaning up\n",
      "Step 1, Loss= 41.1274  h_value: 0.0005245209\n",
      "Step 2, Loss= 0.9636  h_value: 0.015470505\n",
      "Step 3, Loss= 0.7612  h_value: 0.010302544\n",
      "Step 4, Loss= 0.7251  h_value: 0.0060710907\n",
      "Step 5, Loss= 0.7463  h_value: 0.0037488937\n",
      "Step 6, Loss= 0.7206  h_value: 0.0023202896\n",
      "Step 7, Loss= 0.7094  h_value: 0.0015296936\n",
      "Step 8, Loss= 0.7516  h_value: 0.0010919571\n",
      "Step 9, Loss= 0.6972  h_value: 0.0007991791\n",
      "Step 10, Loss= 0.7303  h_value: 0.000623703\n",
      "Step 11, Loss= 0.6950  h_value: 0.0005683899\n",
      "Step 12, Loss= 0.6955  h_value: 0.00046157837\n",
      "Step 13, Loss= 0.7081  h_value: 0.00040721893\n",
      "Step 14, Loss= 0.6863  h_value: 0.00038814545\n",
      "Step 15, Loss= 0.6853  h_value: 0.00039100647\n",
      "Step 16, Loss= 0.6795  h_value: 0.00034713745\n",
      "Step 17, Loss= 0.7174  h_value: 0.0002708435\n",
      "Step 18, Loss= 0.6853  h_value: 0.00028514862\n",
      "Step 19, Loss= 0.6982  h_value: 0.0002670288\n",
      "Step 20, Loss= 0.7003  h_value: 0.00021648407\n",
      "Step 21, Loss= 0.6991  h_value: 0.00023937225\n",
      "Step 22, Loss= 0.7178  h_value: 0.00025177002\n",
      "Step 23, Loss= 0.7056  h_value: 0.00026130676\n",
      "Step 24, Loss= 0.6946  h_value: 0.00022029877\n",
      "Step 25, Loss= 0.7031  h_value: 0.00024032593\n",
      "Step 26, Loss= 0.6910  h_value: 0.0002002716\n",
      "Step 27, Loss= 0.6983  h_value: 0.00016975403\n",
      "Step 28, Loss= 0.6869  h_value: 0.00019645691\n",
      "Step 29, Loss= 0.6859  h_value: 0.0002155304\n",
      "Step 30, Loss= 0.7408  h_value: 0.00020694733\n",
      "Step 31, Loss= 0.6818  h_value: 0.00019359589\n",
      "Step 32, Loss= 0.6877  h_value: 0.00017166138\n",
      "Step 33, Loss= 0.7014  h_value: 0.00021076202\n",
      "Step 34, Loss= 0.7001  h_value: 0.00020313263\n",
      "Step 35, Loss= 0.6916  h_value: 0.00017642975\n",
      "Step 36, Loss= 0.6888  h_value: 0.00018787384\n",
      "Step 37, Loss= 0.7169  h_value: 0.00023365021\n",
      "Step 38, Loss= 0.6936  h_value: 0.00020503998\n",
      "Step 39, Loss= 0.6914  h_value: 0.00023174286\n",
      "Step 40, Loss= 0.6848  h_value: 0.00021648407\n",
      "Step 41, Loss= 0.6882  h_value: 0.0002155304\n",
      "Step 42, Loss= 0.7020  h_value: 0.00021648407\n",
      "Step 43, Loss= 0.6941  h_value: 0.00020694733\n",
      "Step 44, Loss= 0.7106  h_value: 0.00021743774\n",
      "Step 45, Loss= 0.6983  h_value: 0.00019741058\n",
      "Step 46, Loss= 0.6923  h_value: 0.00019454956\n",
      "Step 47, Loss= 0.6876  h_value: 0.00026130676\n",
      "Step 48, Loss= 0.6954  h_value: 0.0002374649\n",
      "Step 49, Loss= 0.6869  h_value: 0.0002374649\n",
      "Step 50, Loss= 0.6901  h_value: 0.0002822876\n",
      "Saving model\n",
      "Step 51, Loss= 0.6966  h_value: 0.00023460388\n",
      "Saving model\n",
      "Step 52, Loss= 0.7000  h_value: 0.00019741058\n",
      "Step 53, Loss= 0.7149  h_value: 0.00025844574\n",
      "Step 54, Loss= 0.7486  h_value: 0.00023651123\n",
      "Saving model\n",
      "Step 55, Loss= 0.6856  h_value: 0.00018119812\n",
      "Saving model\n",
      "Step 56, Loss= 0.6850  h_value: 0.00028800964\n",
      "Step 57, Loss= 0.7105  h_value: 0.00025558472\n",
      "Step 58, Loss= 0.6941  h_value: 0.00024414062\n",
      "Step 59, Loss= 0.6887  h_value: 0.00025749207\n",
      "Step 60, Loss= 0.6874  h_value: 0.000289917\n",
      "Step 61, Loss= 0.7025  h_value: 0.00022697449\n",
      "Step 62, Loss= 0.6877  h_value: 0.00028419495\n",
      "Saving model\n",
      "Step 63, Loss= 0.7010  h_value: 0.00026607513\n",
      "Step 64, Loss= 0.6881  h_value: 0.00025177002\n",
      "Step 65, Loss= 0.6854  h_value: 0.00028800964\n",
      "Step 66, Loss= 0.6865  h_value: 0.0002632141\n",
      "Step 67, Loss= 0.6946  h_value: 0.00031757355\n",
      "Step 68, Loss= 0.7114  h_value: 0.00027656555\n",
      "Step 69, Loss= 0.6863  h_value: 0.0002593994\n",
      "Step 70, Loss= 0.6856  h_value: 0.0002450943\n",
      "Step 71, Loss= 0.7076  h_value: 0.00022792816\n",
      "Step 72, Loss= 0.6883  h_value: 0.00024986267\n",
      "Step 73, Loss= 0.6767  h_value: 0.00025749207\n",
      "Step 74, Loss= 0.6883  h_value: 0.00026607513\n",
      "Step 75, Loss= 0.6831  h_value: 0.0002641678\n",
      "Step 76, Loss= 0.6873  h_value: 0.00025081635\n",
      "Step 77, Loss= 0.6801  h_value: 0.00028038025\n",
      "Step 78, Loss= 0.6819  h_value: 0.00026512146\n",
      "Step 79, Loss= 0.7033  h_value: 0.00026607513\n",
      "Step 80, Loss= 0.6767  h_value: 0.00029563904\n",
      "Step 81, Loss= 0.6905  h_value: 0.00024223328\n",
      "Step 82, Loss= 0.6791  h_value: 0.00029182434\n",
      "Step 83, Loss= 0.6998  h_value: 0.00025558472\n",
      "Step 84, Loss= 0.6963  h_value: 0.00020885468\n",
      "Step 85, Loss= 0.7273  h_value: 0.00023460388\n",
      "Step 86, Loss= 0.6820  h_value: 0.00024795532\n",
      "Step 87, Loss= 0.6773  h_value: 0.00022697449\n",
      "Step 88, Loss= 0.6779  h_value: 0.0002193451\n",
      "Step 89, Loss= 0.6744  h_value: 0.0002117157\n",
      "Step 90, Loss= 0.6752  h_value: 0.00022506714\n",
      "Step 91, Loss= 0.6672  h_value: 0.00022983551\n",
      "Step 92, Loss= 0.6928  h_value: 0.00022792816\n",
      "Step 93, Loss= 0.6722  h_value: 0.00019645691\n",
      "Early stopping\n",
      "INFO:tensorflow:Restoring parameters from tmp.ckpt\n",
      "[[0.    0.007 0.025 0.014 0.004 0.032 0.032 0.014 0.054 0.067]\n",
      " [0.013 0.    0.073 0.073 0.065 0.018 0.011 0.011 0.004 0.01 ]\n",
      " [0.116 0.044 0.    0.034 0.036 0.158 0.005 0.016 0.003 0.007]\n",
      " [0.086 0.048 0.037 0.    0.042 0.02  0.118 0.142 0.007 0.006]\n",
      " [0.015 0.049 0.027 0.023 0.    0.017 0.01  0.009 0.004 0.012]\n",
      " [0.027 0.007 0.019 0.004 0.006 0.    0.009 0.008 0.007 0.016]\n",
      " [0.028 0.005 0.005 0.011 0.002 0.007 0.    0.017 0.016 0.182]\n",
      " [0.023 0.007 0.006 0.011 0.006 0.006 0.022 0.    0.013 0.013]\n",
      " [0.025 0.003 0.006 0.007 0.009 0.004 0.009 0.008 0.    0.015]\n",
      " [0.028 0.007 0.005 0.006 0.005 0.016 0.077 0.015 0.005 0.   ]]\n",
      "MSE =  0.690043503805875\n",
      "0.6609358904758265 0.025079629448775103\n",
      "fold =  7\n",
      "******* Doing dataset size =  5000 ****************\n",
      "Destructor Called... Cleaning up\n",
      "Step 1, Loss= 13.9443  h_value: 0.0005245209\n",
      "Step 2, Loss= 0.7550  h_value: 0.0024909973\n",
      "Step 3, Loss= 0.7191  h_value: 0.001200676\n",
      "Step 4, Loss= 0.7316  h_value: 0.0005788803\n",
      "Step 5, Loss= 0.7311  h_value: 0.00038528442\n",
      "Step 6, Loss= 0.7055  h_value: 0.0002670288\n",
      "Step 7, Loss= 0.7340  h_value: 0.00024700165\n",
      "Step 8, Loss= 0.7045  h_value: 0.00019454956\n",
      "Step 9, Loss= 0.7185  h_value: 0.00015449524\n",
      "Step 10, Loss= 0.6863  h_value: 0.00017642975\n",
      "Step 11, Loss= 0.6859  h_value: 0.00018310547\n",
      "Step 12, Loss= 0.6925  h_value: 0.00019741058\n",
      "Step 13, Loss= 0.6843  h_value: 0.00017261505\n",
      "Step 14, Loss= 0.6926  h_value: 0.00016307831\n",
      "Step 15, Loss= 0.7039  h_value: 0.0001745224\n",
      "Step 16, Loss= 0.6901  h_value: 0.00018692017\n",
      "Step 17, Loss= 0.6790  h_value: 0.00018310547\n",
      "Step 18, Loss= 0.6807  h_value: 0.00018405914\n",
      "Step 19, Loss= 0.6938  h_value: 0.00013160706\n",
      "Step 20, Loss= 0.6877  h_value: 0.00014400482\n",
      "Step 21, Loss= 0.7179  h_value: 0.00014686584\n",
      "Step 22, Loss= 0.7011  h_value: 0.0001411438\n",
      "Step 23, Loss= 0.7055  h_value: 0.00014972687\n",
      "Step 24, Loss= 0.7183  h_value: 0.00014209747\n",
      "Step 25, Loss= 0.6918  h_value: 0.00013637543\n",
      "Step 26, Loss= 0.7065  h_value: 0.00019931793\n",
      "Step 27, Loss= 0.6820  h_value: 0.00015735626\n",
      "Step 28, Loss= 0.6912  h_value: 0.00015354156\n",
      "Step 29, Loss= 0.7179  h_value: 0.00019931793\n",
      "Step 30, Loss= 0.6822  h_value: 0.00019645691\n",
      "Step 31, Loss= 0.6907  h_value: 0.00017642975\n",
      "Step 32, Loss= 0.6929  h_value: 0.00020694733\n",
      "Step 33, Loss= 0.6835  h_value: 0.00019359589\n",
      "Step 34, Loss= 0.6817  h_value: 0.00016880035\n",
      "Step 35, Loss= 0.7077  h_value: 0.00018501282\n",
      "Step 36, Loss= 0.6863  h_value: 0.00019550323\n",
      "Step 37, Loss= 0.6781  h_value: 0.00018787384\n",
      "Step 38, Loss= 0.6812  h_value: 0.00022029877\n",
      "Step 39, Loss= 0.7017  h_value: 0.00021076202\n",
      "Step 40, Loss= 0.6855  h_value: 0.00018692017\n",
      "Step 41, Loss= 0.6766  h_value: 0.0001821518\n",
      "Step 42, Loss= 0.7067  h_value: 0.00016212463\n",
      "Step 43, Loss= 0.6743  h_value: 0.0001707077\n",
      "Step 44, Loss= 0.6983  h_value: 0.00020122528\n",
      "Step 45, Loss= 0.6792  h_value: 0.00017929077\n",
      "Step 46, Loss= 0.6927  h_value: 0.00021266937\n",
      "Step 47, Loss= 0.7072  h_value: 0.00016784668\n",
      "Step 48, Loss= 0.6782  h_value: 0.00020313263\n",
      "Step 49, Loss= 0.7267  h_value: 0.00018024445\n",
      "Step 50, Loss= 0.6884  h_value: 0.0001487732\n",
      "Saving model\n",
      "Step 51, Loss= 0.6958  h_value: 0.00017738342\n",
      "Saving model\n",
      "Step 52, Loss= 0.6854  h_value: 0.00014305115\n",
      "Saving model\n",
      "Step 53, Loss= 0.6756  h_value: 0.00016880035\n",
      "Step 54, Loss= 0.7023  h_value: 0.00014686584\n",
      "Step 55, Loss= 0.6994  h_value: 0.00017929077\n",
      "Step 56, Loss= 0.7156  h_value: 0.00024032593\n",
      "Step 57, Loss= 0.6808  h_value: 0.00019359589\n",
      "Step 58, Loss= 0.7078  h_value: 0.00019454956\n",
      "Step 59, Loss= 0.7003  h_value: 0.00019645691\n",
      "Saving model\n",
      "Step 60, Loss= 0.6792  h_value: 0.00015354156\n",
      "Saving model\n",
      "Step 61, Loss= 0.6724  h_value: 0.0001449585\n",
      "Step 62, Loss= 0.6779  h_value: 0.00020122528\n",
      "Saving model\n",
      "Step 63, Loss= 0.6747  h_value: 0.00017642975\n",
      "Step 64, Loss= 0.6900  h_value: 0.00015830994\n",
      "Step 65, Loss= 0.6751  h_value: 0.00019836426\n",
      "Step 66, Loss= 0.6787  h_value: 0.00014591217\n",
      "Step 67, Loss= 0.6844  h_value: 0.0002040863\n",
      "Step 68, Loss= 0.6796  h_value: 0.00018405914\n",
      "Step 69, Loss= 0.6798  h_value: 0.00017547607\n",
      "Step 70, Loss= 0.6880  h_value: 0.00017642975\n",
      "Step 71, Loss= 0.7022  h_value: 0.00014591217\n",
      "Step 72, Loss= 0.6762  h_value: 0.00015830994\n",
      "Step 73, Loss= 0.7057  h_value: 0.00019264221\n",
      "Step 74, Loss= 0.6983  h_value: 0.00017356873\n",
      "Step 75, Loss= 0.6797  h_value: 0.00013828278\n",
      "Step 76, Loss= 0.6910  h_value: 0.00016975403\n",
      "Step 77, Loss= 0.6791  h_value: 0.00013923645\n",
      "Step 78, Loss= 0.6701  h_value: 0.00015258789\n",
      "Step 79, Loss= 0.6830  h_value: 0.00016212463\n",
      "Step 80, Loss= 0.6823  h_value: 0.00015830994\n",
      "Step 81, Loss= 0.6768  h_value: 0.0001373291\n",
      "Step 82, Loss= 0.6873  h_value: 0.000166893\n",
      "Step 83, Loss= 0.6787  h_value: 0.0001335144\n",
      "Step 84, Loss= 0.6783  h_value: 0.00016975403\n",
      "Step 85, Loss= 0.6801  h_value: 0.00013542175\n",
      "Step 86, Loss= 0.6755  h_value: 0.00013828278\n",
      "Step 87, Loss= 0.6842  h_value: 0.00015735626\n",
      "Step 88, Loss= 0.6794  h_value: 0.00013828278\n",
      "Step 89, Loss= 0.6796  h_value: 0.00016498566\n",
      "Step 90, Loss= 0.6802  h_value: 0.00015544891\n",
      "Step 91, Loss= 0.6832  h_value: 0.0001449585\n",
      "Step 92, Loss= 0.6902  h_value: 0.000113487244\n",
      "Step 93, Loss= 0.6787  h_value: 0.0001039505\n",
      "Early stopping\n",
      "INFO:tensorflow:Restoring parameters from tmp.ckpt\n",
      "[[0.    0.002 0.018 0.019 0.002 0.037 0.029 0.007 0.051 0.068]\n",
      " [0.012 0.    0.057 0.059 0.068 0.008 0.011 0.011 0.003 0.01 ]\n",
      " [0.097 0.04  0.    0.029 0.027 0.126 0.003 0.013 0.001 0.004]\n",
      " [0.089 0.043 0.038 0.    0.035 0.013 0.1   0.124 0.004 0.009]\n",
      " [0.017 0.039 0.032 0.034 0.    0.005 0.01  0.011 0.007 0.008]\n",
      " [0.035 0.003 0.013 0.005 0.004 0.    0.009 0.009 0.008 0.007]\n",
      " [0.032 0.002 0.004 0.009 0.002 0.014 0.    0.013 0.01  0.135]\n",
      " [0.015 0.005 0.003 0.017 0.002 0.014 0.019 0.    0.006 0.01 ]\n",
      " [0.028 0.007 0.004 0.002 0.002 0.007 0.011 0.007 0.    0.017]\n",
      " [0.038 0.004 0.002 0.005 0.003 0.009 0.084 0.004 0.01  0.   ]]\n",
      "MSE =  0.6307661245001625\n",
      "0.6566259239078744 0.025506612520889264\n",
      "fold =  8\n",
      "******* Doing dataset size =  5000 ****************\n",
      "Destructor Called... Cleaning up\n",
      "Step 1, Loss= 60.8065  h_value: 0.0005245209\n",
      "Step 2, Loss= 1.2846  h_value: 0.015649796\n",
      "Step 3, Loss= 0.8374  h_value: 0.010355949\n",
      "Step 4, Loss= 0.8140  h_value: 0.006447792\n",
      "Step 5, Loss= 0.7502  h_value: 0.0039749146\n",
      "Step 6, Loss= 0.7716  h_value: 0.002462387\n",
      "Step 7, Loss= 0.7035  h_value: 0.0014953613\n",
      "Step 8, Loss= 0.7261  h_value: 0.0009469986\n",
      "Step 9, Loss= 0.7147  h_value: 0.0006227493\n",
      "Step 10, Loss= 0.7481  h_value: 0.0004911423\n",
      "Step 11, Loss= 0.7228  h_value: 0.0003643036\n",
      "Step 12, Loss= 0.7021  h_value: 0.00025844574\n",
      "Step 13, Loss= 0.6969  h_value: 0.00019836426\n",
      "Step 14, Loss= 0.7200  h_value: 0.00025081635\n",
      "Step 15, Loss= 0.7231  h_value: 0.00017166138\n",
      "Step 16, Loss= 0.6832  h_value: 0.00015735626\n",
      "Step 17, Loss= 0.7260  h_value: 0.00015735626\n",
      "Step 18, Loss= 0.6892  h_value: 0.00015640259\n",
      "Step 19, Loss= 0.7194  h_value: 0.00018119812\n",
      "Step 20, Loss= 0.7133  h_value: 0.0001487732\n",
      "Step 21, Loss= 0.7517  h_value: 0.00012874603\n",
      "Step 22, Loss= 0.7248  h_value: 0.00013256073\n",
      "Step 23, Loss= 0.7123  h_value: 0.000104904175\n",
      "Step 24, Loss= 0.6951  h_value: 0.00011253357\n",
      "Step 25, Loss= 0.7429  h_value: 9.727478e-05\n",
      "Step 26, Loss= 0.6785  h_value: 0.00011920929\n",
      "Step 27, Loss= 0.6826  h_value: 9.1552734e-05\n",
      "Step 28, Loss= 0.7024  h_value: 0.00013256073\n",
      "Step 29, Loss= 0.6836  h_value: 0.00012111664\n",
      "Step 30, Loss= 0.6818  h_value: 0.00011539459\n",
      "Step 31, Loss= 0.6777  h_value: 0.00012207031\n",
      "Step 32, Loss= 0.7440  h_value: 0.00010967255\n",
      "Step 33, Loss= 0.6774  h_value: 0.00014305115\n",
      "Step 34, Loss= 0.7179  h_value: 0.00014686584\n",
      "Step 35, Loss= 0.6941  h_value: 0.0001373291\n",
      "Step 36, Loss= 0.7231  h_value: 0.00015354156\n",
      "Step 37, Loss= 0.6798  h_value: 0.00012397766\n",
      "Step 38, Loss= 0.6862  h_value: 0.00012779236\n",
      "Step 39, Loss= 0.6763  h_value: 0.00015544891\n",
      "Step 40, Loss= 0.6773  h_value: 0.00015735626\n",
      "Step 41, Loss= 0.6821  h_value: 0.00016498566\n",
      "Step 42, Loss= 0.6840  h_value: 0.00019550323\n",
      "Step 43, Loss= 0.7061  h_value: 0.00018596649\n",
      "Step 44, Loss= 0.6744  h_value: 0.0002002716\n",
      "Step 45, Loss= 0.6942  h_value: 0.0001821518\n",
      "Step 46, Loss= 0.6841  h_value: 0.00022220612\n",
      "Step 47, Loss= 0.7059  h_value: 0.00026607513\n",
      "Step 48, Loss= 0.6906  h_value: 0.0002527237\n",
      "Step 49, Loss= 0.6801  h_value: 0.00022315979\n",
      "Step 50, Loss= 0.6964  h_value: 0.0001783371\n",
      "Saving model\n",
      "Step 51, Loss= 0.6929  h_value: 0.00025558472\n",
      "Step 52, Loss= 0.6943  h_value: 0.0002374649\n",
      "Step 53, Loss= 0.6953  h_value: 0.00025749207\n",
      "Step 54, Loss= 0.6962  h_value: 0.0002593994\n",
      "Step 55, Loss= 0.6843  h_value: 0.00022029877\n",
      "Saving model\n",
      "Step 56, Loss= 0.6893  h_value: 0.0002641678\n",
      "Step 57, Loss= 0.6964  h_value: 0.00024318695\n",
      "Step 58, Loss= 0.6917  h_value: 0.0002784729\n",
      "Step 59, Loss= 0.7030  h_value: 0.00028324127\n",
      "Step 60, Loss= 0.6889  h_value: 0.00025749207\n",
      "Saving model\n",
      "Step 61, Loss= 0.6994  h_value: 0.00022983551\n",
      "Saving model\n",
      "Step 62, Loss= 0.6824  h_value: 0.00026988983\n",
      "Saving model\n",
      "Step 63, Loss= 0.6820  h_value: 0.00023269653\n",
      "Step 64, Loss= 0.6816  h_value: 0.0002641678\n",
      "Step 65, Loss= 0.6990  h_value: 0.00021743774\n",
      "Step 66, Loss= 0.7135  h_value: 0.00022602081\n",
      "Saving model\n",
      "Step 67, Loss= 0.6787  h_value: 0.00026512146\n",
      "Step 68, Loss= 0.6799  h_value: 0.00025558472\n",
      "Step 69, Loss= 0.6969  h_value: 0.00021743774\n",
      "Step 70, Loss= 0.6937  h_value: 0.00019264221\n",
      "Step 71, Loss= 0.7047  h_value: 0.00025463104\n",
      "Step 72, Loss= 0.6804  h_value: 0.00021648407\n",
      "Step 73, Loss= 0.6858  h_value: 0.00031757355\n",
      "Step 74, Loss= 0.6834  h_value: 0.00025749207\n",
      "Step 75, Loss= 0.6779  h_value: 0.00028514862\n",
      "Step 76, Loss= 0.6816  h_value: 0.00025558472\n",
      "Step 77, Loss= 0.6901  h_value: 0.00026226044\n",
      "Step 78, Loss= 0.6771  h_value: 0.00022220612\n",
      "Step 79, Loss= 0.6884  h_value: 0.0002527237\n",
      "Step 80, Loss= 0.6915  h_value: 0.00025558472\n",
      "Step 81, Loss= 0.6962  h_value: 0.00025844574\n",
      "Step 82, Loss= 0.6786  h_value: 0.00023078918\n",
      "Step 83, Loss= 0.6919  h_value: 0.00028705597\n",
      "Step 84, Loss= 0.6859  h_value: 0.0002822876\n",
      "Step 85, Loss= 0.6881  h_value: 0.00028419495\n",
      "Step 86, Loss= 0.6864  h_value: 0.000248909\n",
      "Step 87, Loss= 0.6796  h_value: 0.00026988983\n",
      "Step 88, Loss= 0.6963  h_value: 0.00022315979\n",
      "Step 89, Loss= 0.6980  h_value: 0.00022125244\n",
      "Step 90, Loss= 0.6901  h_value: 0.00020122528\n",
      "Step 91, Loss= 0.6803  h_value: 0.00022506714\n",
      "Step 92, Loss= 0.6870  h_value: 0.00022125244\n",
      "Step 93, Loss= 0.6883  h_value: 0.0002155304\n",
      "Step 94, Loss= 0.7023  h_value: 0.00022506714\n",
      "Step 95, Loss= 0.6793  h_value: 0.00024700165\n",
      "Step 96, Loss= 0.6905  h_value: 0.00025844574\n",
      "Step 97, Loss= 0.7053  h_value: 0.00019264221\n",
      "Early stopping\n",
      "INFO:tensorflow:Restoring parameters from tmp.ckpt\n",
      "[[0.    0.003 0.017 0.012 0.008 0.032 0.037 0.013 0.048 0.068]\n",
      " [0.009 0.    0.065 0.058 0.065 0.003 0.004 0.015 0.003 0.008]\n",
      " [0.084 0.034 0.    0.027 0.027 0.121 0.009 0.011 0.005 0.007]\n",
      " [0.068 0.04  0.025 0.    0.036 0.008 0.087 0.108 0.003 0.006]\n",
      " [0.012 0.034 0.021 0.026 0.    0.007 0.002 0.02  0.004 0.01 ]\n",
      " [0.041 0.004 0.013 0.005 0.003 0.    0.016 0.011 0.006 0.006]\n",
      " [0.028 0.002 0.005 0.016 0.007 0.011 0.    0.016 0.006 0.148]\n",
      " [0.036 0.004 0.003 0.016 0.005 0.01  0.021 0.    0.004 0.018]\n",
      " [0.022 0.005 0.005 0.003 0.003 0.003 0.005 0.009 0.    0.009]\n",
      " [0.021 0.004 0.003 0.003 0.01  0.004 0.103 0.01  0.006 0.   ]]\n",
      "MSE =  0.6576658438865484\n",
      "0.6567559139052087 0.02386172989957529\n",
      "fold =  9\n",
      "******* Doing dataset size =  5000 ****************\n",
      "Destructor Called... Cleaning up\n",
      "Step 1, Loss= 4.2709  h_value: 0.0005245209\n",
      "Step 2, Loss= 0.7773  h_value: 0.00048351288\n",
      "Step 3, Loss= 0.7406  h_value: 0.00016975403\n",
      "Step 4, Loss= 0.7219  h_value: 0.00015258789\n",
      "Step 5, Loss= 0.7023  h_value: 0.00013065338\n",
      "Step 6, Loss= 0.7047  h_value: 0.00014019012\n",
      "Step 7, Loss= 0.6839  h_value: 0.00011253357\n",
      "Step 8, Loss= 0.6842  h_value: 0.00011539459\n",
      "Step 9, Loss= 0.6955  h_value: 0.00016593933\n",
      "Step 10, Loss= 0.7000  h_value: 0.00015068054\n",
      "Step 11, Loss= 0.6771  h_value: 0.00013065338\n",
      "Step 12, Loss= 0.6894  h_value: 0.00013446808\n",
      "Step 13, Loss= 0.6815  h_value: 0.00015354156\n",
      "Step 14, Loss= 0.6778  h_value: 0.00015640259\n",
      "Step 15, Loss= 0.6849  h_value: 0.00016307831\n",
      "Step 16, Loss= 0.6909  h_value: 0.00016784668\n",
      "Step 17, Loss= 0.6720  h_value: 0.00018024445\n",
      "Step 18, Loss= 0.6876  h_value: 0.00019741058\n",
      "Step 19, Loss= 0.6740  h_value: 0.00016975403\n",
      "Step 20, Loss= 0.6766  h_value: 0.00016498566\n",
      "Step 21, Loss= 0.6793  h_value: 0.00018119812\n",
      "Step 22, Loss= 0.6800  h_value: 0.00017738342\n",
      "Step 23, Loss= 0.6790  h_value: 0.00014686584\n",
      "Step 24, Loss= 0.6795  h_value: 0.00015449524\n",
      "Step 25, Loss= 0.6972  h_value: 0.00017166138\n",
      "Step 26, Loss= 0.6808  h_value: 0.00015068054\n",
      "Step 27, Loss= 0.6891  h_value: 0.00016403198\n",
      "Step 28, Loss= 0.6930  h_value: 0.00015258789\n",
      "Step 29, Loss= 0.6888  h_value: 0.00015354156\n",
      "Step 30, Loss= 0.6960  h_value: 0.00016403198\n",
      "Step 31, Loss= 0.6957  h_value: 0.00016212463\n",
      "Step 32, Loss= 0.6914  h_value: 0.00016784668\n",
      "Step 33, Loss= 0.6814  h_value: 0.00017356873\n",
      "Step 34, Loss= 0.6781  h_value: 0.00015640259\n",
      "Step 35, Loss= 0.7062  h_value: 0.00018882751\n",
      "Step 36, Loss= 0.6821  h_value: 0.00015640259\n",
      "Step 37, Loss= 0.6811  h_value: 0.0001707077\n",
      "Step 38, Loss= 0.6915  h_value: 0.00016212463\n",
      "Step 39, Loss= 0.6841  h_value: 0.00013446808\n",
      "Step 40, Loss= 0.6714  h_value: 0.00018310547\n",
      "Step 41, Loss= 0.6843  h_value: 0.00015544891\n",
      "Step 42, Loss= 0.6744  h_value: 0.00017642975\n",
      "Step 43, Loss= 0.7016  h_value: 0.000166893\n",
      "Step 44, Loss= 0.6903  h_value: 0.00014400482\n",
      "Step 45, Loss= 0.6970  h_value: 0.00015068054\n",
      "Step 46, Loss= 0.6785  h_value: 0.00017166138\n",
      "Step 47, Loss= 0.6751  h_value: 0.00017356873\n",
      "Step 48, Loss= 0.6796  h_value: 0.0001487732\n",
      "Step 49, Loss= 0.6915  h_value: 0.00015926361\n",
      "Step 50, Loss= 0.6809  h_value: 0.00019454956\n",
      "Saving model\n",
      "Step 51, Loss= 0.6815  h_value: 0.00013065338\n",
      "Step 52, Loss= 0.6893  h_value: 0.00013923645\n",
      "Saving model\n",
      "Step 53, Loss= 0.6759  h_value: 0.00013065338\n",
      "Saving model\n",
      "Step 54, Loss= 0.6799  h_value: 0.00015640259\n",
      "Step 55, Loss= 0.6888  h_value: 0.00016021729\n",
      "Saving model\n",
      "Step 56, Loss= 0.6760  h_value: 0.00016021729\n",
      "Saving model\n",
      "Step 57, Loss= 0.6764  h_value: 0.00015163422\n",
      "Step 58, Loss= 0.6839  h_value: 0.00014781952\n",
      "Step 59, Loss= 0.6968  h_value: 0.00017738342\n",
      "Saving model\n",
      "Step 60, Loss= 0.6853  h_value: 0.00019454956\n",
      "Step 61, Loss= 0.6878  h_value: 0.00015544891\n",
      "Step 62, Loss= 0.6733  h_value: 0.0001707077\n",
      "Step 63, Loss= 0.6741  h_value: 0.00014019012\n",
      "Step 64, Loss= 0.6738  h_value: 0.00015258789\n",
      "Step 65, Loss= 0.6781  h_value: 0.00016880035\n",
      "Step 66, Loss= 0.6713  h_value: 0.00015163422\n",
      "Step 67, Loss= 0.6986  h_value: 0.00015258789\n",
      "Step 68, Loss= 0.6904  h_value: 0.000166893\n",
      "Saving model\n",
      "Step 69, Loss= 0.6756  h_value: 0.00017166138\n",
      "Step 70, Loss= 0.6713  h_value: 0.00015544891\n",
      "Step 71, Loss= 0.6709  h_value: 0.00015068054\n",
      "Saving model\n",
      "Step 72, Loss= 0.6763  h_value: 0.00014686584\n",
      "Step 73, Loss= 0.6797  h_value: 0.0001411438\n",
      "Step 74, Loss= 0.6746  h_value: 0.00015354156\n",
      "Step 75, Loss= 0.6778  h_value: 0.0001487732\n",
      "Step 76, Loss= 0.6699  h_value: 0.00016117096\n",
      "Step 77, Loss= 0.6706  h_value: 0.00016593933\n",
      "Step 78, Loss= 0.6690  h_value: 0.000166893\n",
      "Step 79, Loss= 0.6855  h_value: 0.0001487732\n",
      "Step 80, Loss= 0.6793  h_value: 0.00015068054\n",
      "Step 81, Loss= 0.6739  h_value: 0.00012397766\n",
      "Step 82, Loss= 0.6780  h_value: 0.00014305115\n",
      "Step 83, Loss= 0.6690  h_value: 0.00012874603\n",
      "Step 84, Loss= 0.6714  h_value: 0.00012683868\n",
      "Step 85, Loss= 0.6776  h_value: 0.00012302399\n",
      "Step 86, Loss= 0.6697  h_value: 0.00012493134\n",
      "Step 87, Loss= 0.7162  h_value: 0.00014209747\n",
      "Step 88, Loss= 0.6767  h_value: 0.00011062622\n",
      "Step 89, Loss= 0.6718  h_value: 0.00012493134\n",
      "Step 90, Loss= 0.6815  h_value: 0.00013446808\n",
      "Step 91, Loss= 0.6945  h_value: 0.00011539459\n",
      "Step 92, Loss= 0.6681  h_value: 0.00011444092\n",
      "Step 93, Loss= 0.7019  h_value: 0.0001077652\n",
      "Step 94, Loss= 0.6794  h_value: 0.000120162964\n",
      "Step 95, Loss= 0.6794  h_value: 0.00011634827\n",
      "Step 96, Loss= 0.6846  h_value: 0.00011730194\n",
      "Step 97, Loss= 0.6749  h_value: 0.0001335144\n",
      "Saving model\n",
      "Step 98, Loss= 0.6714  h_value: 0.0001296997\n",
      "Step 99, Loss= 0.6719  h_value: 0.000118255615\n",
      "Step 100, Loss= 0.6711  h_value: 0.0001039505\n",
      "Step 101, Loss= 0.6770  h_value: 0.00011253357\n",
      "Step 102, Loss= 0.6764  h_value: 0.00012874603\n",
      "Step 103, Loss= 0.6844  h_value: 0.00013542175\n",
      "Step 104, Loss= 0.6808  h_value: 0.00011539459\n",
      "Saving model\n",
      "Step 105, Loss= 0.6777  h_value: 0.00010871887\n",
      "Step 106, Loss= 0.6806  h_value: 0.000120162964\n",
      "Step 107, Loss= 0.6744  h_value: 0.00012779236\n",
      "Step 108, Loss= 0.6664  h_value: 0.00013637543\n",
      "Step 109, Loss= 0.6837  h_value: 0.0001373291\n",
      "Step 110, Loss= 0.6764  h_value: 0.00013542175\n",
      "Step 111, Loss= 0.6938  h_value: 0.00011253357\n",
      "Step 112, Loss= 0.6660  h_value: 0.00012779236\n",
      "Step 113, Loss= 0.6718  h_value: 0.00012683868\n",
      "Step 114, Loss= 0.6732  h_value: 0.00012588501\n",
      "Step 115, Loss= 0.6701  h_value: 0.00012111664\n",
      "Step 116, Loss= 0.6762  h_value: 0.00012111664\n",
      "Step 117, Loss= 0.6763  h_value: 0.00012302399\n",
      "Step 118, Loss= 0.7092  h_value: 0.00010108948\n",
      "Step 119, Loss= 0.6770  h_value: 0.00011444092\n",
      "Step 120, Loss= 0.6715  h_value: 0.00010585785\n",
      "Step 121, Loss= 0.6754  h_value: 0.00011539459\n",
      "Step 122, Loss= 0.6909  h_value: 9.8228455e-05\n",
      "Step 123, Loss= 0.6707  h_value: 9.346008e-05\n",
      "Step 124, Loss= 0.6791  h_value: 0.00011920929\n",
      "Step 125, Loss= 0.6710  h_value: 9.441376e-05\n",
      "Step 126, Loss= 0.6985  h_value: 0.00010204315\n",
      "Step 127, Loss= 0.7011  h_value: 0.00011634827\n",
      "Step 128, Loss= 0.6914  h_value: 0.00010108948\n",
      "Step 129, Loss= 0.6707  h_value: 9.8228455e-05\n",
      "Step 130, Loss= 0.6765  h_value: 0.00010108948\n",
      "Step 131, Loss= 0.6910  h_value: 9.441376e-05\n",
      "Step 132, Loss= 0.6718  h_value: 9.1552734e-05\n",
      "Step 133, Loss= 0.6743  h_value: 0.00010585785\n",
      "Step 134, Loss= 0.6759  h_value: 0.0001077652\n",
      "Step 135, Loss= 0.6716  h_value: 9.441376e-05\n",
      "Early stopping\n",
      "INFO:tensorflow:Restoring parameters from tmp.ckpt\n",
      "[[0.    0.003 0.02  0.014 0.003 0.03  0.015 0.006 0.051 0.058]\n",
      " [0.006 0.    0.065 0.062 0.065 0.004 0.006 0.005 0.005 0.007]\n",
      " [0.097 0.032 0.    0.019 0.015 0.119 0.006 0.01  0.005 0.007]\n",
      " [0.094 0.04  0.023 0.    0.02  0.008 0.097 0.106 0.005 0.01 ]\n",
      " [0.003 0.04  0.018 0.018 0.    0.007 0.006 0.001 0.004 0.012]\n",
      " [0.038 0.004 0.018 0.007 0.003 0.    0.01  0.004 0.004 0.015]\n",
      " [0.009 0.003 0.005 0.007 0.003 0.014 0.    0.008 0.004 0.124]\n",
      " [0.019 0.005 0.006 0.01  0.005 0.009 0.015 0.    0.004 0.016]\n",
      " [0.028 0.003 0.002 0.003 0.005 0.007 0.008 0.009 0.    0.004]\n",
      " [0.025 0.005 0.003 0.003 0.005 0.005 0.069 0.022 0.004 0.   ]]\n",
      "MSE =  0.6550784507025075\n",
      "0.6565695291049086 0.02250323053241382\n",
      "fold =  10\n",
      "******* Doing dataset size =  5000 ****************\n",
      "Destructor Called... Cleaning up\n",
      "Step 1, Loss= 19.7783  h_value: 0.0005245209\n",
      "Step 2, Loss= 0.7491  h_value: 0.0026426315\n",
      "Step 3, Loss= 0.7141  h_value: 0.0012083054\n",
      "Step 4, Loss= 0.7186  h_value: 0.0006008148\n",
      "Step 5, Loss= 0.7470  h_value: 0.0003824234\n",
      "Step 6, Loss= 0.7316  h_value: 0.00023555756\n",
      "Step 7, Loss= 0.7248  h_value: 0.00021076202\n",
      "Step 8, Loss= 0.7277  h_value: 0.00018692017\n",
      "Step 9, Loss= 0.7321  h_value: 0.00016593933\n",
      "Step 10, Loss= 0.7397  h_value: 0.00020599365\n",
      "Step 11, Loss= 0.7054  h_value: 0.00015068054\n",
      "Step 12, Loss= 0.7034  h_value: 0.00016403198\n",
      "Step 13, Loss= 0.6933  h_value: 0.00018310547\n",
      "Step 14, Loss= 0.7063  h_value: 0.0001335144\n",
      "Step 15, Loss= 0.6995  h_value: 0.00015354156\n",
      "Step 16, Loss= 0.7037  h_value: 0.00017642975\n",
      "Step 17, Loss= 0.7043  h_value: 0.00018405914\n",
      "Step 18, Loss= 0.7188  h_value: 0.00017356873\n",
      "Step 19, Loss= 0.7186  h_value: 0.00017738342\n",
      "Step 20, Loss= 0.7239  h_value: 0.0002117157\n",
      "Step 21, Loss= 0.7092  h_value: 0.000207901\n",
      "Step 22, Loss= 0.6843  h_value: 0.00018882751\n",
      "Step 23, Loss= 0.6928  h_value: 0.00019168854\n",
      "Step 24, Loss= 0.6867  h_value: 0.00015449524\n",
      "Step 25, Loss= 0.6956  h_value: 0.0002002716\n",
      "Step 26, Loss= 0.6843  h_value: 0.00020313263\n",
      "Step 27, Loss= 0.6900  h_value: 0.0002155304\n",
      "Step 28, Loss= 0.6981  h_value: 0.0002117157\n",
      "Step 29, Loss= 0.6883  h_value: 0.00018119812\n",
      "Step 30, Loss= 0.6989  h_value: 0.00018882751\n",
      "Step 31, Loss= 0.6874  h_value: 0.0001821518\n",
      "Step 32, Loss= 0.6977  h_value: 0.00022125244\n",
      "Step 33, Loss= 0.7023  h_value: 0.0001821518\n",
      "Step 34, Loss= 0.7298  h_value: 0.00017166138\n",
      "Step 35, Loss= 0.6912  h_value: 0.00023937225\n",
      "Step 36, Loss= 0.6833  h_value: 0.00025844574\n",
      "Step 37, Loss= 0.7011  h_value: 0.00021839142\n",
      "Step 38, Loss= 0.7103  h_value: 0.00023174286\n",
      "Step 39, Loss= 0.6940  h_value: 0.00024700165\n",
      "Step 40, Loss= 0.7064  h_value: 0.00023365021\n",
      "Step 41, Loss= 0.6933  h_value: 0.00022983551\n",
      "Step 42, Loss= 0.6896  h_value: 0.00025463104\n",
      "Step 43, Loss= 0.7109  h_value: 0.00023841858\n",
      "Step 44, Loss= 0.6850  h_value: 0.00022315979\n",
      "Step 45, Loss= 0.7124  h_value: 0.00023078918\n",
      "Step 46, Loss= 0.6866  h_value: 0.00022602081\n",
      "Step 47, Loss= 0.6865  h_value: 0.00020217896\n",
      "Step 48, Loss= 0.6958  h_value: 0.00024318695\n",
      "Step 49, Loss= 0.6840  h_value: 0.000248909\n",
      "Step 50, Loss= 0.6881  h_value: 0.00023269653\n",
      "Saving model\n",
      "Step 51, Loss= 0.6804  h_value: 0.00020599365\n",
      "Step 52, Loss= 0.6977  h_value: 0.00022220612\n",
      "Step 53, Loss= 0.6849  h_value: 0.00024414062\n",
      "Step 54, Loss= 0.6946  h_value: 0.0002117157\n",
      "Step 55, Loss= 0.6864  h_value: 0.00018119812\n",
      "Step 56, Loss= 0.7014  h_value: 0.00022029877\n",
      "Step 57, Loss= 0.6921  h_value: 0.00023651123\n",
      "Step 58, Loss= 0.7031  h_value: 0.00020599365\n",
      "Step 59, Loss= 0.6816  h_value: 0.0002632141\n",
      "Step 60, Loss= 0.6905  h_value: 0.00023365021\n",
      "Step 61, Loss= 0.6865  h_value: 0.00022506714\n",
      "Step 62, Loss= 0.6773  h_value: 0.0002565384\n",
      "Saving model\n",
      "Step 63, Loss= 0.6872  h_value: 0.00025558472\n",
      "Step 64, Loss= 0.6965  h_value: 0.00022602081\n",
      "Saving model\n",
      "Step 65, Loss= 0.6786  h_value: 0.00021266937\n",
      "Saving model\n",
      "Step 66, Loss= 0.6897  h_value: 0.00026607513\n",
      "Step 67, Loss= 0.6950  h_value: 0.00023841858\n",
      "Step 68, Loss= 0.6808  h_value: 0.00028133392\n",
      "Step 69, Loss= 0.6824  h_value: 0.000207901\n",
      "Step 70, Loss= 0.7229  h_value: 0.00019836426\n",
      "Step 71, Loss= 0.6887  h_value: 0.0002450943\n",
      "Step 72, Loss= 0.7038  h_value: 0.00022697449\n",
      "Step 73, Loss= 0.6766  h_value: 0.0002117157\n",
      "Step 74, Loss= 0.6851  h_value: 0.00018787384\n",
      "Step 75, Loss= 0.6921  h_value: 0.00019264221\n",
      "Step 76, Loss= 0.7001  h_value: 0.00019931793\n",
      "Step 77, Loss= 0.6728  h_value: 0.00018596649\n",
      "Step 78, Loss= 0.6888  h_value: 0.0002117157\n",
      "Step 79, Loss= 0.6877  h_value: 0.00019454956\n",
      "Step 80, Loss= 0.7024  h_value: 0.00020122528\n",
      "Step 81, Loss= 0.6907  h_value: 0.00019454956\n",
      "Saving model\n",
      "Step 82, Loss= 0.6795  h_value: 0.00019454956\n",
      "Step 83, Loss= 0.6757  h_value: 0.00015830994\n",
      "Step 84, Loss= 0.6993  h_value: 0.00017929077\n",
      "Step 85, Loss= 0.6926  h_value: 0.0001821518\n",
      "Step 86, Loss= 0.6894  h_value: 0.00017642975\n",
      "Step 87, Loss= 0.6920  h_value: 0.00017738342\n",
      "Step 88, Loss= 0.6918  h_value: 0.00015926361\n",
      "Step 89, Loss= 0.6927  h_value: 0.00019168854\n",
      "Step 90, Loss= 0.6892  h_value: 0.00016117096\n",
      "Step 91, Loss= 0.6893  h_value: 0.00014686584\n",
      "Step 92, Loss= 0.6792  h_value: 0.00013828278\n",
      "Step 93, Loss= 0.6705  h_value: 0.00020217896\n",
      "Step 94, Loss= 0.6833  h_value: 0.00016975403\n",
      "Step 95, Loss= 0.6986  h_value: 0.00016212463\n",
      "Step 96, Loss= 0.6833  h_value: 0.00016880035\n",
      "Step 97, Loss= 0.6927  h_value: 0.0001707077\n",
      "Step 98, Loss= 0.6868  h_value: 0.000166893\n",
      "Step 99, Loss= 0.6947  h_value: 0.00016403198\n",
      "Step 100, Loss= 0.6745  h_value: 0.00018310547\n",
      "Step 101, Loss= 0.6835  h_value: 0.0001411438\n",
      "Step 102, Loss= 0.6981  h_value: 0.00018310547\n",
      "Step 103, Loss= 0.6779  h_value: 0.00016403198\n",
      "Step 104, Loss= 0.6879  h_value: 0.00012111664\n",
      "Step 105, Loss= 0.6901  h_value: 0.00014019012\n",
      "Step 106, Loss= 0.6800  h_value: 0.00015354156\n",
      "Step 107, Loss= 0.6765  h_value: 0.0001487732\n",
      "Step 108, Loss= 0.7292  h_value: 0.00015926361\n",
      "Step 109, Loss= 0.6782  h_value: 0.00015735626\n",
      "Step 110, Loss= 0.6773  h_value: 0.00014591217\n",
      "Step 111, Loss= 0.6966  h_value: 0.00012493134\n",
      "Step 112, Loss= 0.7015  h_value: 0.00013828278\n",
      "Early stopping\n",
      "INFO:tensorflow:Restoring parameters from tmp.ckpt\n",
      "[[0.    0.002 0.017 0.021 0.005 0.045 0.035 0.007 0.053 0.055]\n",
      " [0.008 0.    0.072 0.061 0.07  0.005 0.011 0.008 0.004 0.007]\n",
      " [0.095 0.036 0.    0.031 0.027 0.139 0.009 0.005 0.011 0.008]\n",
      " [0.071 0.04  0.031 0.    0.035 0.003 0.108 0.109 0.011 0.007]\n",
      " [0.013 0.045 0.022 0.027 0.    0.008 0.011 0.004 0.007 0.005]\n",
      " [0.039 0.003 0.022 0.006 0.005 0.    0.009 0.007 0.003 0.014]\n",
      " [0.038 0.004 0.007 0.011 0.007 0.01  0.    0.023 0.003 0.163]\n",
      " [0.024 0.006 0.004 0.01  0.003 0.022 0.028 0.    0.009 0.009]\n",
      " [0.026 0.004 0.006 0.002 0.009 0.004 0.015 0.005 0.    0.014]\n",
      " [0.026 0.004 0.004 0.005 0.004 0.008 0.073 0.004 0.007 0.   ]]\n",
      "MSE =  0.6270070655047935\n",
      "0.653613282744897 0.02311731772557529\n"
     ]
    }
   ],
   "source": [
    "    reg_lambda = 1\n",
    "    reg_beta = 5\n",
    "\n",
    "    fold = 0\n",
    "    REG_castle = []\n",
    "    print(\"Dataset limits are\", np.ptp(X_DAG), np.ptp(X_test), np.ptp(y_test))\n",
    "    for train_idx, val_idx in kf.split(X_DAG):\n",
    "        fold += 1\n",
    "        print(\"fold = \", fold)\n",
    "        print(\"******* Doing dataset size = \", dataset_sz , \"****************\")\n",
    "        X_train = X_DAG[train_idx]\n",
    "        y_train = np.expand_dims(X_DAG[train_idx][:,0], -1)\n",
    "        X_val = X_DAG[val_idx]\n",
    "        y_val = X_DAG[val_idx][:,0]\n",
    "\n",
    "        w_threshold = 0.3\n",
    "        castle = CASTLE(num_train = X_DAG.shape[0], num_inputs = X_DAG.shape[1], reg_lambda = reg_lambda, reg_beta = reg_beta,\n",
    "                            w_threshold = w_threshold)\n",
    "        num_nodes = np.shape(X_DAG)[1]\n",
    "        castle.fit(X_train, y_train, num_nodes, X_val, y_val, X_test, y_test)\n",
    "        W_est = castle.pred_W(X_DAG, np.expand_dims(X_DAG[:,0], -1))\n",
    "        print(W_est)\n",
    "\n",
    "        REG_castle.append(mean_squared_error(castle.pred(X_test), y_test))\n",
    "        print(\"MSE = \", mean_squared_error(castle.pred(X_test), y_test))\n",
    "\n",
    "        if fold > 1:\n",
    "            print(np.mean(REG_castle), np.std(REG_castle))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0\n",
      "0  0.635888\n",
      "1  0.682855\n",
      "2  0.649542\n",
      "3  0.681774\n",
      "4  0.625513\n",
      "5  0.690044\n",
      "6  0.630766\n",
      "7  0.657666\n",
      "8  0.655078\n",
      "9  0.627007\n",
      "Mean=  0.654 , Sd=  0.0231\n"
     ]
    }
   ],
   "source": [
    "print(pd.DataFrame(REG_castle))\n",
    "print(\"Mean= \",\"{:.3}\".format(np.mean(REG_castle)),\", Sd= \",\"{:.3}\".format(np.std(REG_castle)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_full = False\n",
    "if run_full:\n",
    "    scaler = StandardScaler()\n",
    "    n_folds = 10\n",
    "    df = scaler.fit_transform(df)\n",
    "    df_test = scaler.transform(df_test)\n",
    "\n",
    "    X_test = df_test\n",
    "    y_test = df_test[:,0]\n",
    "    X_DAG = df\n",
    "    y_DAG = np.expand_dims(X_DAG[:,0], -1)\n",
    "\n",
    "    castle.fit(X_DAG, y_DAG, num_nodes, X_test, y_test, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_abfb397e_5032_11eb_85b1_38002594084erow0_col0 {\n",
       "            background-color:  #f0f1f3;\n",
       "            color:  #000000;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow0_col1 {\n",
       "            background-color:  #f0f1f3;\n",
       "            color:  #000000;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow0_col2 {\n",
       "            background-color:  #ccd6e0;\n",
       "            color:  #000000;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow0_col3 {\n",
       "            background-color:  #b0c1d1;\n",
       "            color:  #000000;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow0_col4 {\n",
       "            background-color:  #f0f1f3;\n",
       "            color:  #000000;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow0_col5 {\n",
       "            background-color:  #b5c5d4;\n",
       "            color:  #000000;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow0_col6 {\n",
       "            background-color:  #b5c4d3;\n",
       "            color:  #000000;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow0_col7 {\n",
       "            background-color:  #f0f1f3;\n",
       "            color:  #000000;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow0_col8 {\n",
       "            background-color:  #023f75;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow0_col9 {\n",
       "            background-color:  #b2c2d2;\n",
       "            color:  #000000;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow1_col0 {\n",
       "            background-color:  #f0f1f3;\n",
       "            color:  #000000;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow1_col1 {\n",
       "            background-color:  #f0f1f3;\n",
       "            color:  #000000;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow1_col2 {\n",
       "            background-color:  #023f75;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow1_col3 {\n",
       "            background-color:  #023f75;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow1_col4 {\n",
       "            background-color:  #023f75;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow1_col5 {\n",
       "            background-color:  #f0f1f3;\n",
       "            color:  #000000;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow1_col6 {\n",
       "            background-color:  #f0f1f3;\n",
       "            color:  #000000;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow1_col7 {\n",
       "            background-color:  #f0f1f3;\n",
       "            color:  #000000;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow1_col8 {\n",
       "            background-color:  #f0f1f3;\n",
       "            color:  #000000;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow1_col9 {\n",
       "            background-color:  #f0f1f3;\n",
       "            color:  #000000;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow2_col0 {\n",
       "            background-color:  #023f75;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow2_col1 {\n",
       "            background-color:  #376791;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow2_col2 {\n",
       "            background-color:  #f0f1f3;\n",
       "            color:  #000000;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow2_col3 {\n",
       "            background-color:  #85a1ba;\n",
       "            color:  #000000;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow2_col4 {\n",
       "            background-color:  #a5b9cb;\n",
       "            color:  #000000;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow2_col5 {\n",
       "            background-color:  #023f75;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow2_col6 {\n",
       "            background-color:  #f0f1f3;\n",
       "            color:  #000000;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow2_col7 {\n",
       "            background-color:  #f0f1f3;\n",
       "            color:  #000000;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow2_col8 {\n",
       "            background-color:  #d4dce4;\n",
       "            color:  #000000;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow2_col9 {\n",
       "            background-color:  #f0f1f3;\n",
       "            color:  #000000;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow3_col0 {\n",
       "            background-color:  #457198;\n",
       "            color:  #000000;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow3_col1 {\n",
       "            background-color:  #205685;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow3_col2 {\n",
       "            background-color:  #99b0c5;\n",
       "            color:  #000000;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow3_col3 {\n",
       "            background-color:  #f0f1f3;\n",
       "            color:  #000000;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow3_col4 {\n",
       "            background-color:  #86a2bb;\n",
       "            color:  #000000;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow3_col5 {\n",
       "            background-color:  #f0f1f3;\n",
       "            color:  #000000;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow3_col6 {\n",
       "            background-color:  #023f75;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow3_col7 {\n",
       "            background-color:  #023f75;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow3_col8 {\n",
       "            background-color:  #d4dce4;\n",
       "            color:  #000000;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow3_col9 {\n",
       "            background-color:  #f0f1f3;\n",
       "            color:  #000000;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow4_col0 {\n",
       "            background-color:  #e6eaee;\n",
       "            color:  #000000;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow4_col1 {\n",
       "            background-color:  #023f75;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow4_col2 {\n",
       "            background-color:  #bac9d6;\n",
       "            color:  #000000;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow4_col3 {\n",
       "            background-color:  #96adc3;\n",
       "            color:  #000000;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow4_col4 {\n",
       "            background-color:  #f0f1f3;\n",
       "            color:  #000000;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow4_col5 {\n",
       "            background-color:  #f0f1f3;\n",
       "            color:  #000000;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow4_col6 {\n",
       "            background-color:  #f0f1f3;\n",
       "            color:  #000000;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow4_col7 {\n",
       "            background-color:  #f0f1f3;\n",
       "            color:  #000000;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow4_col8 {\n",
       "            background-color:  #e7eaee;\n",
       "            color:  #000000;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow4_col9 {\n",
       "            background-color:  #f0f1f3;\n",
       "            color:  #000000;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow5_col0 {\n",
       "            background-color:  #9eb4c8;\n",
       "            color:  #000000;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow5_col1 {\n",
       "            background-color:  #f0f1f3;\n",
       "            color:  #000000;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow5_col2 {\n",
       "            background-color:  #bac9d6;\n",
       "            color:  #000000;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow5_col3 {\n",
       "            background-color:  #f0f1f3;\n",
       "            color:  #000000;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow5_col4 {\n",
       "            background-color:  #f0f1f3;\n",
       "            color:  #000000;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow5_col5 {\n",
       "            background-color:  #f0f1f3;\n",
       "            color:  #000000;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow5_col6 {\n",
       "            background-color:  #f0f1f3;\n",
       "            color:  #000000;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow5_col7 {\n",
       "            background-color:  #f0f1f3;\n",
       "            color:  #000000;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow5_col8 {\n",
       "            background-color:  #f0f1f3;\n",
       "            color:  #000000;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow5_col9 {\n",
       "            background-color:  #f0f1f3;\n",
       "            color:  #000000;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow6_col0 {\n",
       "            background-color:  #a1b6c9;\n",
       "            color:  #000000;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow6_col1 {\n",
       "            background-color:  #f0f1f3;\n",
       "            color:  #000000;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow6_col2 {\n",
       "            background-color:  #f0f1f3;\n",
       "            color:  #000000;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow6_col3 {\n",
       "            background-color:  #dbe1e8;\n",
       "            color:  #000000;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow6_col4 {\n",
       "            background-color:  #f0f1f3;\n",
       "            color:  #000000;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow6_col5 {\n",
       "            background-color:  #f0f1f3;\n",
       "            color:  #000000;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow6_col6 {\n",
       "            background-color:  #f0f1f3;\n",
       "            color:  #000000;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow6_col7 {\n",
       "            background-color:  #d3dbe3;\n",
       "            color:  #000000;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow6_col8 {\n",
       "            background-color:  #f0f1f3;\n",
       "            color:  #000000;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow6_col9 {\n",
       "            background-color:  #023f75;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow7_col0 {\n",
       "            background-color:  #c8d3de;\n",
       "            color:  #000000;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow7_col1 {\n",
       "            background-color:  #e7eaee;\n",
       "            color:  #000000;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow7_col2 {\n",
       "            background-color:  #f0f1f3;\n",
       "            color:  #000000;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow7_col3 {\n",
       "            background-color:  #e0e5ea;\n",
       "            color:  #000000;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow7_col4 {\n",
       "            background-color:  #f0f1f3;\n",
       "            color:  #000000;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow7_col5 {\n",
       "            background-color:  #e1e5eb;\n",
       "            color:  #000000;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow7_col6 {\n",
       "            background-color:  #c6d2dd;\n",
       "            color:  #000000;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow7_col7 {\n",
       "            background-color:  #f0f1f3;\n",
       "            color:  #000000;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow7_col8 {\n",
       "            background-color:  #dee3e9;\n",
       "            color:  #000000;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow7_col9 {\n",
       "            background-color:  #f0f1f3;\n",
       "            color:  #000000;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow8_col0 {\n",
       "            background-color:  #c3cfdb;\n",
       "            color:  #000000;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow8_col1 {\n",
       "            background-color:  #f0f1f3;\n",
       "            color:  #000000;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow8_col2 {\n",
       "            background-color:  #f0f1f3;\n",
       "            color:  #000000;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow8_col3 {\n",
       "            background-color:  #f0f1f3;\n",
       "            color:  #000000;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow8_col4 {\n",
       "            background-color:  #e8ebef;\n",
       "            color:  #000000;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow8_col5 {\n",
       "            background-color:  #f0f1f3;\n",
       "            color:  #000000;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow8_col6 {\n",
       "            background-color:  #e6eaee;\n",
       "            color:  #000000;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow8_col7 {\n",
       "            background-color:  #f0f1f3;\n",
       "            color:  #000000;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow8_col8 {\n",
       "            background-color:  #f0f1f3;\n",
       "            color:  #000000;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow8_col9 {\n",
       "            background-color:  #f0f1f3;\n",
       "            color:  #000000;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow9_col0 {\n",
       "            background-color:  #c3cfdb;\n",
       "            color:  #000000;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow9_col1 {\n",
       "            background-color:  #f0f1f3;\n",
       "            color:  #000000;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow9_col2 {\n",
       "            background-color:  #f0f1f3;\n",
       "            color:  #000000;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow9_col3 {\n",
       "            background-color:  #f0f1f3;\n",
       "            color:  #000000;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow9_col4 {\n",
       "            background-color:  #f0f1f3;\n",
       "            color:  #000000;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow9_col5 {\n",
       "            background-color:  #f0f1f3;\n",
       "            color:  #000000;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow9_col6 {\n",
       "            background-color:  #577fa2;\n",
       "            color:  #000000;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow9_col7 {\n",
       "            background-color:  #f0f1f3;\n",
       "            color:  #000000;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow9_col8 {\n",
       "            background-color:  #e7eaee;\n",
       "            color:  #000000;\n",
       "        }    #T_abfb397e_5032_11eb_85b1_38002594084erow9_col9 {\n",
       "            background-color:  #f0f1f3;\n",
       "            color:  #000000;\n",
       "        }</style><table id=\"T_abfb397e_5032_11eb_85b1_38002594084e\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >0</th>        <th class=\"col_heading level0 col1\" >1</th>        <th class=\"col_heading level0 col2\" >2</th>        <th class=\"col_heading level0 col3\" >3</th>        <th class=\"col_heading level0 col4\" >4</th>        <th class=\"col_heading level0 col5\" >5</th>        <th class=\"col_heading level0 col6\" >6</th>        <th class=\"col_heading level0 col7\" >7</th>        <th class=\"col_heading level0 col8\" >8</th>        <th class=\"col_heading level0 col9\" >9</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_abfb397e_5032_11eb_85b1_38002594084elevel0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow0_col0\" class=\"data row0 col0\" >0.0</td>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow0_col1\" class=\"data row0 col1\" >0.002</td>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow0_col2\" class=\"data row0 col2\" >0.017</td>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow0_col3\" class=\"data row0 col3\" >0.021</td>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow0_col4\" class=\"data row0 col4\" >0.005</td>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow0_col5\" class=\"data row0 col5\" >0.045</td>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow0_col6\" class=\"data row0 col6\" >0.035</td>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow0_col7\" class=\"data row0 col7\" >0.007</td>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow0_col8\" class=\"data row0 col8\" >0.053</td>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow0_col9\" class=\"data row0 col9\" >0.055</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_abfb397e_5032_11eb_85b1_38002594084elevel0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow1_col0\" class=\"data row1 col0\" >0.008</td>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow1_col1\" class=\"data row1 col1\" >0.0</td>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow1_col2\" class=\"data row1 col2\" >0.072</td>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow1_col3\" class=\"data row1 col3\" >0.061</td>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow1_col4\" class=\"data row1 col4\" >0.07</td>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow1_col5\" class=\"data row1 col5\" >0.005</td>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow1_col6\" class=\"data row1 col6\" >0.011</td>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow1_col7\" class=\"data row1 col7\" >0.008</td>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow1_col8\" class=\"data row1 col8\" >0.004</td>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow1_col9\" class=\"data row1 col9\" >0.007</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_abfb397e_5032_11eb_85b1_38002594084elevel0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow2_col0\" class=\"data row2 col0\" >0.095</td>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow2_col1\" class=\"data row2 col1\" >0.036</td>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow2_col2\" class=\"data row2 col2\" >0.0</td>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow2_col3\" class=\"data row2 col3\" >0.031</td>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow2_col4\" class=\"data row2 col4\" >0.027</td>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow2_col5\" class=\"data row2 col5\" >0.139</td>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow2_col6\" class=\"data row2 col6\" >0.009</td>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow2_col7\" class=\"data row2 col7\" >0.005</td>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow2_col8\" class=\"data row2 col8\" >0.011</td>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow2_col9\" class=\"data row2 col9\" >0.008</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_abfb397e_5032_11eb_85b1_38002594084elevel0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow3_col0\" class=\"data row3 col0\" >0.071</td>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow3_col1\" class=\"data row3 col1\" >0.04</td>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow3_col2\" class=\"data row3 col2\" >0.031</td>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow3_col3\" class=\"data row3 col3\" >0.0</td>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow3_col4\" class=\"data row3 col4\" >0.035</td>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow3_col5\" class=\"data row3 col5\" >0.003</td>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow3_col6\" class=\"data row3 col6\" >0.108</td>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow3_col7\" class=\"data row3 col7\" >0.109</td>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow3_col8\" class=\"data row3 col8\" >0.011</td>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow3_col9\" class=\"data row3 col9\" >0.007</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_abfb397e_5032_11eb_85b1_38002594084elevel0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow4_col0\" class=\"data row4 col0\" >0.013</td>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow4_col1\" class=\"data row4 col1\" >0.045</td>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow4_col2\" class=\"data row4 col2\" >0.022</td>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow4_col3\" class=\"data row4 col3\" >0.027</td>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow4_col4\" class=\"data row4 col4\" >0.0</td>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow4_col5\" class=\"data row4 col5\" >0.008</td>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow4_col6\" class=\"data row4 col6\" >0.011</td>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow4_col7\" class=\"data row4 col7\" >0.004</td>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow4_col8\" class=\"data row4 col8\" >0.007</td>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow4_col9\" class=\"data row4 col9\" >0.005</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_abfb397e_5032_11eb_85b1_38002594084elevel0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow5_col0\" class=\"data row5 col0\" >0.039</td>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow5_col1\" class=\"data row5 col1\" >0.003</td>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow5_col2\" class=\"data row5 col2\" >0.022</td>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow5_col3\" class=\"data row5 col3\" >0.006</td>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow5_col4\" class=\"data row5 col4\" >0.005</td>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow5_col5\" class=\"data row5 col5\" >0.0</td>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow5_col6\" class=\"data row5 col6\" >0.009</td>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow5_col7\" class=\"data row5 col7\" >0.007</td>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow5_col8\" class=\"data row5 col8\" >0.003</td>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow5_col9\" class=\"data row5 col9\" >0.014</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_abfb397e_5032_11eb_85b1_38002594084elevel0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow6_col0\" class=\"data row6 col0\" >0.038</td>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow6_col1\" class=\"data row6 col1\" >0.004</td>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow6_col2\" class=\"data row6 col2\" >0.007</td>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow6_col3\" class=\"data row6 col3\" >0.011</td>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow6_col4\" class=\"data row6 col4\" >0.007</td>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow6_col5\" class=\"data row6 col5\" >0.01</td>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow6_col6\" class=\"data row6 col6\" >0.0</td>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow6_col7\" class=\"data row6 col7\" >0.023</td>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow6_col8\" class=\"data row6 col8\" >0.003</td>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow6_col9\" class=\"data row6 col9\" >0.163</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_abfb397e_5032_11eb_85b1_38002594084elevel0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow7_col0\" class=\"data row7 col0\" >0.024</td>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow7_col1\" class=\"data row7 col1\" >0.006</td>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow7_col2\" class=\"data row7 col2\" >0.004</td>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow7_col3\" class=\"data row7 col3\" >0.01</td>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow7_col4\" class=\"data row7 col4\" >0.003</td>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow7_col5\" class=\"data row7 col5\" >0.022</td>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow7_col6\" class=\"data row7 col6\" >0.028</td>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow7_col7\" class=\"data row7 col7\" >0.0</td>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow7_col8\" class=\"data row7 col8\" >0.009</td>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow7_col9\" class=\"data row7 col9\" >0.009</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_abfb397e_5032_11eb_85b1_38002594084elevel0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow8_col0\" class=\"data row8 col0\" >0.026</td>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow8_col1\" class=\"data row8 col1\" >0.004</td>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow8_col2\" class=\"data row8 col2\" >0.006</td>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow8_col3\" class=\"data row8 col3\" >0.002</td>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow8_col4\" class=\"data row8 col4\" >0.009</td>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow8_col5\" class=\"data row8 col5\" >0.004</td>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow8_col6\" class=\"data row8 col6\" >0.015</td>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow8_col7\" class=\"data row8 col7\" >0.005</td>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow8_col8\" class=\"data row8 col8\" >0.0</td>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow8_col9\" class=\"data row8 col9\" >0.014</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_abfb397e_5032_11eb_85b1_38002594084elevel0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow9_col0\" class=\"data row9 col0\" >0.026</td>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow9_col1\" class=\"data row9 col1\" >0.004</td>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow9_col2\" class=\"data row9 col2\" >0.004</td>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow9_col3\" class=\"data row9 col3\" >0.005</td>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow9_col4\" class=\"data row9 col4\" >0.004</td>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow9_col5\" class=\"data row9 col5\" >0.008</td>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow9_col6\" class=\"data row9 col6\" >0.073</td>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow9_col7\" class=\"data row9 col7\" >0.004</td>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow9_col8\" class=\"data row9 col8\" >0.007</td>\n",
       "                        <td id=\"T_abfb397e_5032_11eb_85b1_38002594084erow9_col9\" class=\"data row9 col9\" >0.0</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x237224ae488>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "intersection_matrix = castle.get_weights(X_train, y_train)\n",
    "cm = sns.light_palette(\"#003E74\", as_cmap=True)\n",
    "x=pd.DataFrame(intersection_matrix).round(3)\n",
    "x=x.style.background_gradient(cmap=cm, low=-0.1, high=0.01).format(\"{:.3}\")\n",
    "display(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\brizio\\AppData\\Roaming\\Python\\Python37\\site-packages\\networkx\\drawing\\layout.py:950: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  pos = np.row_stack((pos[x] for x in node_list))\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pos=nx.planar_layout(G)\n",
    "nx.draw(G,pos)\n",
    "labels={}\n",
    "for i in range(10):\n",
    "    labels[i] = i\n",
    "nx.draw_networkx_labels(G,pos,labels,font_size=16);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G1 = nx.from_numpy_matrix(intersection_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos=nx.circular_layout(G1)\n",
    "nx.draw(G1,pos=pos)\n",
    "labels={}\n",
    "for i in range(10):\n",
    "    labels[i] = i\n",
    "nx.draw_networkx_labels(G,pos,labels,font_size=16);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Castlenv",
   "language": "python",
   "name": "castlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
